{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import gensim\n",
    "from nmt_utils import *\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"ConciseReviews.csv\", sep=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "Summary:  a powerful documentarylike examination of the response to an occupying force the battle of algiers has not aged a bit since its release in 1966 \n",
      "\n",
      "Reviews:  a riveting reconstruction of the struggle for independence in mid-50s algiers, its pseudo-documentary style still feels as headline-fresh as its content. brilliantly directed set-pieces and remarkable thronging crowd scenes make the film a masterpiece; the ominous familiarity of its subject makes it a must-see. gripping, relevant cinemairish film, \"bloody sunday,\" borrowed its entire structure from \"algiers\". \"the battle of algiers\" is a one of a kind masterpiece of pure cinema that you will never forget. the film establishes a kinetic documentary effect, making the impact of every shoot-out and explosion a deeply personal experience. this seminal meticulously crafted work about algiers struggle for freedom won the venic film fest top award, and deservedly garnered pontecorvo oscar nominations as best director and co-scribe. the directors real achievement is not in making a piece of agitprop but in using these fundamental tools of cinema in such an extraordinarily affecting way. while it is clear that the movie is very much a stacked deck, which invariably takes the algerian side, the bias does not lessen the films lesson. remains a signficant achievement, a testament to films powers to put you in the middle of a historical event and make it feel real. pontecorvo has nearly accomplished the impossible: to make an epic film that convinces the viewer he is watching the real thing. vital and disquieting, this is a powerful depiction of the pity of war as shocking today as it was on the day of its release. as something similar happens to our soldiers in iraq, it is a piece of blood-chilling horror every time we see this story from todays news repeated in a 38-year-old movie. a fascinating movie in its own right, more relevant because of todays world situation. staged with almost newsreel precision but presented with the punch of a great political thrillerdespite being half a century old, still has a powerful and pertinent message. \n",
      "\n",
      "\n",
      "\n",
      "Review # 2\n",
      "Summary:  poor plot development and slow pacing keep 54 from capturing the energy of it is legendary namesake \n",
      "\n",
      "Reviews:  qualifies as one of the more entertaining bad movies of the year. a disappointingly dull film. a mish-mash of bad writing, poor acting except myers, amateur directing and mediocre production credits. just as disappointing as whit stillmans last days of disco, marc christophers 54 suffers from a formulaic script that feels like a reworking of saturday night fever and boogie nights, both superior and more entertaining pictures. a nostalgic whitewash with its sterling beat-heavy soundtrack and non-stop glitz, the movie captures the look and sound of its inspiration much more convincingly than it conjures up the studio 54 atmosphere. the 1970s were great times, but most viewers could hardly come to that conclusion based on this film. the new disco era film 54 is fame on drugs, a low-grade saturday night fever. the plotting and pacing are so relentlessly predictable that the film does not even work well as a sociology lesson. here we have a nightclub where mick jagger danced with truman capote, and the drama of the film is two bare-chested hunks shouting youve changed, man! the film is entertaining, and it is worth seeing if only for mike myers amazing performance. little more than stock characters acting out a generic drama of sin and redemption against a lascivious backdrop that is far more interesting than the story it serves. mr christophers movie at times looks like a parody of the era, at other times like a celebration of a period haplessly gone by. the directors cut still has a pacing that lags during the second act, and now that the films about the love triangle, the scenes with julie do not really add anything to the overall story that being said, it deserves a second chance in its new life. \n",
      "\n",
      "\n",
      "\n",
      "Review # 3\n",
      "Summary:  while it hews closely to the 1984 original craig brewer infuses his footloose remake with toetapping energy and manages to keep the story fresh for a new generation \n",
      "\n",
      "Reviews:  hugely entertaining, remake that is obviously been made with a healthy love of the original film and succeeds thanks to a faithful script, terrific dance sequences and strong performances from its cast. wormald and hough are both handsome and good on the dance floor, but they come across more like teen stars in training than representations of real youth angst. updating the quarter-of-a-century old original with a more racially representative cast and hipper street moves, brewer also trumps his predecessor with a zinging script and some likeable performances. wormald and hough do not look or dance like adolescents letting off steam; their professionalism overwhelms what should be a simple pleasure the film sometimes has the same problem. brewer has a firm command of the material and a great sense of how to film dance sequences cannot wait to see what this filmmaker does next. fema can take a day off because footloose is one remake that will not be called a natural disaster, in fact, it is one hell of a good film. its unlikely that brewers footloose will persuade many remake skeptics but the film does deliver surprisingly enjoyable character drama and exciting choreography. its taken me a long time to come to grips with this cinematic weakness - or maybe it is a blessing i like dance movies, especially if the music is bouncy and it makes the characters happy that is an infectious film sequence for me. brewer can  stage and shoot a dance sequence in a way that the movement can be wholly appreciated, and those scenes become infectiously entertaining. its a rare remake that improves on the original but the new footloose does just that the dancing is better, the drama grittier, the stars sexier -- and the music and the cast are more energetic and diverse. with his delightful learning-how-to-dance sequence, miles teller almost steals footloose from fantastic dancers kenny wormald and julianne hough. an almost risibly old-fashioned crowd-pleaserweirdly constrained by brewers determination to ape the original so earnestlyso clean-cut and wholesome that the mtv imprimatur attached to it seems like a joke. footloose is completely harmless, and quite enjoyable at times like any good pop song, it is got a beat, and you can dance to it. the movie plays like a slightly degraded version of the original: the dialogue is a little lamer, the acting a little poorer. dont hate footloose because it is a remake the dance-movie-love-story-freedom-of-expression thumper is a rollicking good time. tweaking the story just a little, the new film brings things up to date while including a few nostalgic salutes to the original - including rens bright-yellow vw beetle and ariels red-leather cowboy boots. the good-looking, athletic cast kick up their heels with gusto and choreographer jamal sims melds line dancing with hip hop and street, always harking back affectionately to the iconic moments of the earlier version. craig brewer directs films like a man with electricity emanating from his fingertips it feels like his pictures might, at any point, ignite into a fiery ball of sex  footloose is what we talk about when we talk about having fun at the movies. brewer makes good use of the songs from the 1984 film that work, and while the film is too long, dragging out the drama, it is got a real pulse. barely fresh considering director craig brewer essentially made a carbon copy of the original film just with less chemistry and a leading actor who sounded like mark whalberg!. foot-tapping fun, sexy dancing, romance and making changes are the mainstays of this likeable, high energy remake of the 1984 classic. footloose is a fun, energetic and sexy remake which successfully remixes its cult classic predecessor to make for an entertaining time at the flicks. from start to finish, brewers remake exudes the look and style of its forebearers: semi-awkward dance choreography, clunky dialogue and an obedience to formula that borders on clich but somehow, it works. brewer, who previously put his high-intensity spin on hustle & flow and black snake moan, displays his coolest moves in the way he smartly unties this footloose from its 1980s moorings. brewer puts just enough smarts, sweat and swagger into his version of the dance steps making up this film that you cannot help but move your feet and hum along. while hewing closely to footlooses original story and themes, brewers film throws the standard high school movie notion of a teenage caste system out the window. while it is true that few viewers will venture into a theater showing this remake for the story, the screenplay should not be a detriment to enjoyment less talking and more dancing would have made for a more footloose and fancy-free environment. after his sophomore jinx, brewer is back on firm ground with this retro but not cool, old-fashioned musical remake, which you can see with your parents and grandparents due to its unabashedly corny and harmless nature. it feels like a film walking in the shoes of its predecessor with a slightly sassier spring in its step but at a certain point in the final act, this remake seems to forget why we are all watching. the glue subtly holding everything together for footloose is the intelligent and atmospheric direction of craig brewer, best known for his superb handling of the cult classic hustle and flow. it may adhere to the original a little too closely, but this remake is still an immensely enjoyable ride boosted by craig brewers pitch-perfect musical instincts. nothing about this remake challenges the original what it does do is introduce a new generation to a great story with heart and there really cannot be anything wrong with that. \n",
      "\n",
      "\n",
      "\n",
      "Review # 4\n",
      "Summary:  tender funny and touching the sessions provides an acting showcase for its talented stars and proves it is possible for hollywood to produce a grownup movie about sex \n",
      "\n",
      "Reviews:  this frank, funny, tender film both asks and receives more from its sex scenes than any movie i have seen in a long time. \"the sessions\" is a pleasant shock: a touching, profoundly sex-positive film that equates sex with intimacy, tenderness and emotional connection instead of performance, competition and conquest. john hawkes does the kind of acting that awards were invented for in this exhilarating gift of a movie that is funny, touching and vital. a beautifully touching true story that never feels like the typical biopic since it finds a way of instilling humor and charm into a delicate and often tragic situation. john hawkes delivers what is perhaps the performance of his career and helen hunt is fearless and charming the sessions is a heartfelt journey that will leave your emotional spectrum overwhelmed to your hearts content. john hawkes and helen hunt generate an endearing chemistry, here, turning in a couple of virtuoso performances deserving of serious consideration come oscar season. the honest performances and assured direction makes the sessions an extremely accomplished film that celebrates sexuality. with ben lewins film the sessions, hawkes is given the biggest and juiciest leading role of his career, and he pulls it off with remarkable grace and humor. ben lewins the sessions is an emotionally charged film with a smart sense of humor thatll have you tearing up from equal doses of laughter and drama. the sessions is a tender, well-intentioned examination of a disabled man exploring his sexual identity, but sometimes a film needs more than just good intentions. its the rare film to sell sex as something truly tender and life-affirming, and helen hunt, in particular, is lovely and poignant. in ben lewins the sessions, john hawkes takes on the kind of role that earns academy awards, in the kind of film that doesnt. aided by committed, awards-ready performance, the sessions transforms taboo subject matter into a humorous, humane and uncomplicated pleasure. this is not the first film to explore the subject of the sexuality of the disabled but it is, thanks to sensitive direction from ben lewin and empathetic performances that never condescend, among the best. its a brave performance from hunt, who spends much of the film entirely naked both her and hawkes are brilliant in a movie that is a massively uplifting experience. the movie becomes a touching, often funny portrayal of sex as a form of kindness and human contact. its an adult movie about an adult subject that upends the conventions of the sex comedy by investing them with solemn purpose fortunately, that turns out to be pretty funny in itself. it has some difficult and heartfelt performances as well as moments of uncomfortable honesty, but ultimately writer-director ben lewins film feels too slight, too pat, and too wildly overhyped out of its festival showings. sex, religion, poetry and the disabled might be the subject matter, but nothing will prepare you for the emotions that are shared in this unique, poignant and unforgettable film. a story of triumph over disability that takes the form of an uplifting sex comedy rather than a depressing saga of a dying man. at once entirely frank and downright cuddly in the way it deals with the seldom-visited subject of the sex lives of people with disabilities. the sessions belongs completely to hawkes, who disappears into the role of mark obrien, delivering a stunning performance that illuminates what it means to be a whole person. hawkes performance is the must-see hook of the sessions, but hunt gives this funny, touching movie its soul. character actor john hawkes is often cast as a frightening rustic winters bone, martha marcy may marlene, but he gives a tender and witty performance here as mark obrien. hugely enjoyable, warm-hearted and frequently laugh-out-loud funny disability drama with a superb script and a pair of terrific performances from john hawkes and helen hunt. a touching gem of a movie largely thanks to subtly dynamic performances from helen hunt and john hawkes. an incredibly low-key and feel-good adult movie about sex that is much funnier than most people are probably expecting, highlighted by a pair of oscar-caliber performances by its two leads. neither an issue-pushing disability drama or a crude, american polio-style sex comedy, the sessions is sweet and winning - feel good minus the fingers down the throat. a frank exploration of sex and disability, the sessions compensates for a minor structural misstep with an acute ear for tone and stellar performances throughout. a film, inspired by the life of the late poet-journalist mark obrien, that celebrates the relationship between physical and emotional intimacy. not just another weepy drama of overcoming odds, a my left foot with a different appendage the sessions is often brazenly funny, not from shocking dialogue but characters reacting the way people do, especially with such a flustering subject as sex. take away the nudity and the frank sex talk and you would pretty much be left with a high-minded tv movie -- with unusually good actors. inevitable comparison will be made with my left foot, but on its own terms, the fact-based drama is touching, frank, challenging breaking hollywood taboos, and superbly acted by hawkes and helen hunt in oscar-caliber turns. the sessions is bracing it is also one of the few movies to recognize that people with severe physical disabilities have sexual lives, too. the sessions is a magnificent film this tale of a profoundly disabled mans sexual awaking is not only a genuine crowd pleaser , but it also gives a cinematic voice to the marginalized. a movie like this is really built around a performance, and john hawkes - so powerfully evil in \"winters bone\" and \"martha marcy may marlene\" - carries the film. deeply moving the oscar-worthy john hawkes invests his character with a sense of grace and humor that nullifies any potential pity this is a great movie for adults and even for older adolescents. \n",
      "\n",
      "\n",
      "\n",
      "Review # 5\n",
      "Summary:  patrice chreaus exquisite rendering of joseph conrads the return brings underlying passions to surface in a longsuffering marriage \n",
      "\n",
      "Reviews:  a film that matches all too well the times it portrays, gabrielle is claustrophobic, stifling, and not a little crusty saved only by its exquisitely bitter performances and immaculate design. for the most part, chereau lets huppert and greggory provide the emotional impact they respond accordingly, imbuing their mutual suffering with an exacting and moving finesse. as a couple, jean and gabrielle are a corseted waking nightmare as co-stars, isabelle huppert and pascal greggory make a dream match. patrice chreaus startling drama recounts the dissolution of a marriage of a couple pascal greggory and isabelle huppert from the haute bourgeoisie. greggory anchors gabrielle in manly bewilderment and rage, while huppert claws the title characters way to self-awareness. though gabrielle could be described as a very talky film, probing talk is a french specialty, especially when it explores the dangerous, ever-changing chambers of the heart. isabelle huppert and pascal greggory are superb as a couple of immense wealth and social prestige in the belle epoque paris of 1912 -- but then everything about this film is superb. as loyal as chreaus film is to conrads story, the director expands its point of view by giving more authority to the female experience conrad suppresses in his text. much of the film is huppert and greggory verbally eviscerating their life together, not to mention their dreams and beliefs, and watching the give and take is like watching great theatre. husband and wife, upper-class couple jean and gabrielle hervey, are played, to perfection, by two of frances premier film actors: pascal greggory and isabelle huppert. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(\"Summary: \",reviews.TargetSummary[i],\"\\n\")\n",
    "    print(\"Reviews: \",reviews.ConciseMovieReview[i],\"\\n\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks to energetic performances from its young leads gimme the loot captures a slice of city life with warmth and exuberance'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.TargetSummary[182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtractedReviews=[]\n",
    "Summaries=[]\n",
    "ts=[]\n",
    "for i in range(840):\n",
    "    ExtractedReviews.append(reviews.ConciseMovieReview[i])\n",
    "    ts.append(reviews.TargetSummary[i])\n",
    "    Summaries.append('SOS '+reviews.TargetSummary[i]+' EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOS a powerful documentarylike examination of the response to an occupying force the battle of algiers has not aged a bit since its release in 1966 EOS',\n",
       " 'SOS poor plot development and slow pacing keep 54 from capturing the energy of it is legendary namesake EOS',\n",
       " 'SOS while it hews closely to the 1984 original craig brewer infuses his footloose remake with toetapping energy and manages to keep the story fresh for a new generation EOS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summaries[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedtokenizer=Tokenizer(char_level=False)\n",
    "extractedtokenizer.fit_on_texts(ExtractedReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedWord_counts=extractedtokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', 16613),\n",
       "             ('riveting', 80),\n",
       "             ('reconstruction', 3),\n",
       "             ('of', 14158),\n",
       "             ('the', 24059),\n",
       "             ('struggle', 34),\n",
       "             ('for', 3339),\n",
       "             ('independence', 14),\n",
       "             ('in', 6743),\n",
       "             ('mid', 18),\n",
       "             ('50s', 9),\n",
       "             ('algiers', 5),\n",
       "             ('its', 2823),\n",
       "             ('pseudo', 8),\n",
       "             ('documentary', 426),\n",
       "             ('style', 254),\n",
       "             ('still', 446),\n",
       "             ('feels', 351),\n",
       "             ('as', 4084),\n",
       "             ('headline', 4),\n",
       "             ('fresh', 116),\n",
       "             ('content', 33),\n",
       "             ('brilliantly', 73),\n",
       "             ('directed', 215),\n",
       "             ('set', 248),\n",
       "             ('pieces', 65),\n",
       "             ('and', 14937),\n",
       "             ('remarkable', 93),\n",
       "             ('thronging', 1),\n",
       "             ('crowd', 77),\n",
       "             ('scenes', 240),\n",
       "             ('make', 716),\n",
       "             ('film', 3919),\n",
       "             ('masterpiece', 104),\n",
       "             ('ominous', 5),\n",
       "             ('familiarity', 10),\n",
       "             ('subject', 146),\n",
       "             ('makes', 704),\n",
       "             ('it', 5937),\n",
       "             ('must', 128),\n",
       "             ('see', 415),\n",
       "             ('gripping', 94),\n",
       "             ('relevant', 27),\n",
       "             ('cinemairish', 1),\n",
       "             ('bloody', 59),\n",
       "             ('sunday', 19),\n",
       "             ('borrowed', 4),\n",
       "             ('entire', 56),\n",
       "             ('structure', 35),\n",
       "             ('from', 1751),\n",
       "             ('battle', 92),\n",
       "             ('is', 11338),\n",
       "             ('one', 1729),\n",
       "             ('kind', 275),\n",
       "             ('pure', 73),\n",
       "             ('cinema', 172),\n",
       "             ('that', 5541),\n",
       "             ('you', 2029),\n",
       "             ('will', 1132),\n",
       "             ('never', 454),\n",
       "             ('forget', 54),\n",
       "             ('establishes', 7),\n",
       "             ('kinetic', 15),\n",
       "             ('effect', 50),\n",
       "             ('making', 321),\n",
       "             ('impact', 54),\n",
       "             ('every', 314),\n",
       "             ('shoot', 15),\n",
       "             ('out', 926),\n",
       "             ('explosion', 4),\n",
       "             ('deeply', 102),\n",
       "             ('personal', 106),\n",
       "             ('experience', 207),\n",
       "             ('this', 3077),\n",
       "             ('seminal', 14),\n",
       "             ('meticulously', 11),\n",
       "             ('crafted', 110),\n",
       "             ('work', 522),\n",
       "             ('about', 1571),\n",
       "             ('freedom', 16),\n",
       "             ('won', 26),\n",
       "             ('venic', 1),\n",
       "             ('fest', 22),\n",
       "             ('top', 167),\n",
       "             ('award', 40),\n",
       "             ('deservedly', 8),\n",
       "             ('garnered', 2),\n",
       "             ('pontecorvo', 2),\n",
       "             ('oscar', 198),\n",
       "             ('nominations', 6),\n",
       "             ('best', 1113),\n",
       "             ('director', 1184),\n",
       "             ('co', 87),\n",
       "             ('scribe', 5),\n",
       "             ('directors', 116),\n",
       "             ('real', 455),\n",
       "             ('achievement', 45),\n",
       "             ('not', 3042),\n",
       "             ('piece', 163),\n",
       "             ('agitprop', 5),\n",
       "             ('but', 3498),\n",
       "             ('using', 45),\n",
       "             ('these', 194),\n",
       "             ('fundamental', 6),\n",
       "             ('tools', 6),\n",
       "             ('such', 311),\n",
       "             ('an', 2688),\n",
       "             ('extraordinarily', 9),\n",
       "             ('affecting', 41),\n",
       "             ('way', 667),\n",
       "             ('while', 559),\n",
       "             ('clear', 68),\n",
       "             ('movie', 3113),\n",
       "             ('very', 450),\n",
       "             ('much', 735),\n",
       "             ('stacked', 3),\n",
       "             ('deck', 1),\n",
       "             ('which', 675),\n",
       "             ('invariably', 5),\n",
       "             ('takes', 281),\n",
       "             ('algerian', 2),\n",
       "             ('side', 65),\n",
       "             ('bias', 4),\n",
       "             ('does', 850),\n",
       "             ('lessen', 1),\n",
       "             ('films', 1063),\n",
       "             ('lesson', 46),\n",
       "             ('remains', 96),\n",
       "             ('signficant', 1),\n",
       "             ('testament', 23),\n",
       "             ('to', 8829),\n",
       "             ('powers', 19),\n",
       "             ('put', 115),\n",
       "             ('middle', 75),\n",
       "             ('historical', 77),\n",
       "             ('event', 39),\n",
       "             ('feel', 390),\n",
       "             ('has', 1606),\n",
       "             ('nearly', 113),\n",
       "             ('accomplished', 41),\n",
       "             ('impossible', 110),\n",
       "             ('epic', 174),\n",
       "             ('convinces', 2),\n",
       "             ('viewer', 57),\n",
       "             ('he', 968),\n",
       "             ('watching', 205),\n",
       "             ('thing', 241),\n",
       "             ('vital', 30),\n",
       "             ('disquieting', 5),\n",
       "             ('powerful', 171),\n",
       "             ('depiction', 32),\n",
       "             ('pity', 12),\n",
       "             ('war', 271),\n",
       "             ('shocking', 34),\n",
       "             ('today', 56),\n",
       "             ('was', 527),\n",
       "             ('on', 2221),\n",
       "             ('day', 204),\n",
       "             ('release', 36),\n",
       "             ('something', 341),\n",
       "             ('similar', 27),\n",
       "             ('happens', 54),\n",
       "             ('our', 220),\n",
       "             ('soldiers', 38),\n",
       "             ('iraq', 36),\n",
       "             ('blood', 81),\n",
       "             ('chilling', 29),\n",
       "             ('horror', 346),\n",
       "             ('time', 863),\n",
       "             ('we', 492),\n",
       "             ('story', 1325),\n",
       "             ('todays', 26),\n",
       "             ('news', 50),\n",
       "             ('repeated', 7),\n",
       "             ('38', 2),\n",
       "             ('year', 452),\n",
       "             ('old', 473),\n",
       "             ('fascinating', 164),\n",
       "             ('own', 287),\n",
       "             ('right', 309),\n",
       "             ('more', 1350),\n",
       "             ('because', 266),\n",
       "             ('world', 486),\n",
       "             ('situation', 30),\n",
       "             ('staged', 23),\n",
       "             ('with', 4079),\n",
       "             ('almost', 254),\n",
       "             ('newsreel', 2),\n",
       "             ('precision', 16),\n",
       "             ('presented', 16),\n",
       "             ('punch', 57),\n",
       "             ('great', 638),\n",
       "             ('political', 222),\n",
       "             ('thrillerdespite', 1),\n",
       "             ('being', 274),\n",
       "             ('half', 178),\n",
       "             ('century', 59),\n",
       "             ('pertinent', 4),\n",
       "             ('message', 111),\n",
       "             ('qualifies', 11),\n",
       "             ('entertaining', 412),\n",
       "             ('bad', 353),\n",
       "             ('movies', 866),\n",
       "             ('disappointingly', 13),\n",
       "             ('dull', 73),\n",
       "             ('mish', 2),\n",
       "             ('mash', 6),\n",
       "             ('writing', 69),\n",
       "             ('poor', 31),\n",
       "             ('acting', 231),\n",
       "             ('except', 40),\n",
       "             ('myers', 8),\n",
       "             ('amateur', 7),\n",
       "             ('directing', 62),\n",
       "             ('mediocre', 29),\n",
       "             ('production', 87),\n",
       "             ('credits', 26),\n",
       "             ('just', 861),\n",
       "             ('disappointing', 38),\n",
       "             ('whit', 3),\n",
       "             ('stillmans', 1),\n",
       "             ('last', 261),\n",
       "             ('days', 125),\n",
       "             ('disco', 3),\n",
       "             ('marc', 5),\n",
       "             ('christophers', 2),\n",
       "             ('54', 3),\n",
       "             ('suffers', 37),\n",
       "             ('formulaic', 57),\n",
       "             ('script', 321),\n",
       "             ('like', 1864),\n",
       "             ('reworking', 3),\n",
       "             ('saturday', 21),\n",
       "             ('night', 154),\n",
       "             ('fever', 16),\n",
       "             ('boogie', 2),\n",
       "             ('nights', 26),\n",
       "             ('both', 384),\n",
       "             ('superior', 29),\n",
       "             ('pictures', 62),\n",
       "             ('nostalgic', 21),\n",
       "             ('whitewash', 2),\n",
       "             ('sterling', 7),\n",
       "             ('beat', 34),\n",
       "             ('heavy', 53),\n",
       "             ('soundtrack', 30),\n",
       "             ('non', 59),\n",
       "             ('stop', 60),\n",
       "             ('glitz', 1),\n",
       "             ('captures', 69),\n",
       "             ('look', 387),\n",
       "             ('sound', 80),\n",
       "             ('inspiration', 29),\n",
       "             ('convincingly', 8),\n",
       "             ('than', 1269),\n",
       "             ('conjures', 7),\n",
       "             ('up', 933),\n",
       "             ('studio', 55),\n",
       "             ('atmosphere', 38),\n",
       "             ('1970s', 25),\n",
       "             ('were', 187),\n",
       "             ('times', 216),\n",
       "             ('most', 1107),\n",
       "             ('viewers', 96),\n",
       "             ('could', 404),\n",
       "             ('hardly', 50),\n",
       "             ('come', 280),\n",
       "             ('conclusion', 35),\n",
       "             ('based', 166),\n",
       "             ('new', 671),\n",
       "             ('era', 78),\n",
       "             ('fame', 8),\n",
       "             ('drugs', 14),\n",
       "             ('low', 126),\n",
       "             ('grade', 18),\n",
       "             ('plotting', 25),\n",
       "             ('pacing', 49),\n",
       "             ('are', 1597),\n",
       "             ('so', 1100),\n",
       "             ('relentlessly', 23),\n",
       "             ('predictable', 103),\n",
       "             ('even', 712),\n",
       "             ('well', 639),\n",
       "             ('sociology', 5),\n",
       "             ('here', 225),\n",
       "             ('have', 1340),\n",
       "             ('nightclub', 4),\n",
       "             ('where', 232),\n",
       "             ('mick', 1),\n",
       "             ('jagger', 1),\n",
       "             ('danced', 1),\n",
       "             ('truman', 3),\n",
       "             ('capote', 1),\n",
       "             ('drama', 551),\n",
       "             ('two', 478),\n",
       "             ('bare', 12),\n",
       "             ('chested', 2),\n",
       "             ('hunks', 2),\n",
       "             ('shouting', 4),\n",
       "             ('youve', 92),\n",
       "             ('changed', 15),\n",
       "             ('man', 500),\n",
       "             ('worth', 174),\n",
       "             ('seeing', 99),\n",
       "             ('if', 1027),\n",
       "             ('only', 581),\n",
       "             ('mike', 53),\n",
       "             ('amazing', 77),\n",
       "             ('performance', 699),\n",
       "             ('little', 584),\n",
       "             ('stock', 19),\n",
       "             ('characters', 628),\n",
       "             ('generic', 27),\n",
       "             ('sin', 61),\n",
       "             ('redemption', 26),\n",
       "             ('against', 109),\n",
       "             ('lascivious', 2),\n",
       "             ('backdrop', 17),\n",
       "             ('far', 322),\n",
       "             ('interesting', 145),\n",
       "             ('serves', 45),\n",
       "             ('mr', 66),\n",
       "             ('at', 1455),\n",
       "             ('looks', 148),\n",
       "             ('parody', 33),\n",
       "             ('other', 325),\n",
       "             ('celebration', 37),\n",
       "             ('period', 85),\n",
       "             ('haplessly', 1),\n",
       "             ('gone', 91),\n",
       "             ('by', 2047),\n",
       "             ('cut', 57),\n",
       "             ('lags', 3),\n",
       "             ('during', 73),\n",
       "             ('second', 154),\n",
       "             ('act', 107),\n",
       "             ('now', 183),\n",
       "             ('love', 563),\n",
       "             ('triangle', 7),\n",
       "             ('julie', 7),\n",
       "             ('do', 588),\n",
       "             ('really', 318),\n",
       "             ('add', 40),\n",
       "             ('anything', 157),\n",
       "             ('overall', 55),\n",
       "             ('said', 51),\n",
       "             ('deserves', 94),\n",
       "             ('chance', 67),\n",
       "             ('life', 629),\n",
       "             ('hugely', 44),\n",
       "             ('remake', 105),\n",
       "             ('obviously', 15),\n",
       "             ('been', 474),\n",
       "             ('made', 561),\n",
       "             ('healthy', 10),\n",
       "             ('original', 361),\n",
       "             ('succeeds', 62),\n",
       "             ('thanks', 165),\n",
       "             ('faithful', 33),\n",
       "             ('terrific', 207),\n",
       "             ('dance', 62),\n",
       "             ('sequences', 133),\n",
       "             ('strong', 211),\n",
       "             ('performances', 532),\n",
       "             ('cast', 371),\n",
       "             ('wormald', 3),\n",
       "             ('hough', 3),\n",
       "             ('handsome', 27),\n",
       "             ('good', 1147),\n",
       "             ('floor', 6),\n",
       "             ('they', 569),\n",
       "             ('across', 75),\n",
       "             ('teen', 116),\n",
       "             ('stars', 144),\n",
       "             ('training', 33),\n",
       "             ('representations', 2),\n",
       "             ('youth', 26),\n",
       "             ('angst', 17),\n",
       "             ('updating', 5),\n",
       "             ('quarter', 8),\n",
       "             ('racially', 4),\n",
       "             ('representative', 3),\n",
       "             ('hipper', 3),\n",
       "             ('street', 53),\n",
       "             ('moves', 45),\n",
       "             ('brewer', 10),\n",
       "             ('also', 421),\n",
       "             ('trumps', 1),\n",
       "             ('his', 1827),\n",
       "             ('predecessor', 47),\n",
       "             ('zinging', 1),\n",
       "             ('some', 706),\n",
       "             ('likeable', 28),\n",
       "             ('or', 911),\n",
       "             ('adolescents', 2),\n",
       "             ('letting', 29),\n",
       "             ('off', 364),\n",
       "             ('steam', 10),\n",
       "             ('their', 658),\n",
       "             ('professionalism', 6),\n",
       "             ('overwhelms', 3),\n",
       "             ('what', 852),\n",
       "             ('should', 326),\n",
       "             ('be', 1881),\n",
       "             ('simple', 110),\n",
       "             ('pleasure', 53),\n",
       "             ('sometimes', 106),\n",
       "             ('same', 198),\n",
       "             ('problem', 75),\n",
       "             ('firm', 11),\n",
       "             ('command', 7),\n",
       "             ('material', 120),\n",
       "             ('sense', 296),\n",
       "             ('how', 463),\n",
       "             ('cannot', 225),\n",
       "             ('wait', 30),\n",
       "             ('filmmaker', 167),\n",
       "             ('next', 88),\n",
       "             ('fema', 1),\n",
       "             ('can', 649),\n",
       "             ('take', 243),\n",
       "             ('footloose', 11),\n",
       "             ('called', 56),\n",
       "             ('natural', 57),\n",
       "             ('disaster', 50),\n",
       "             ('fact', 120),\n",
       "             ('hell', 55),\n",
       "             ('unlikely', 38),\n",
       "             ('brewers', 5),\n",
       "             ('persuade', 3),\n",
       "             ('many', 377),\n",
       "             ('skeptics', 6),\n",
       "             ('deliver', 76),\n",
       "             ('surprisingly', 107),\n",
       "             ('enjoyable', 176),\n",
       "             ('character', 464),\n",
       "             ('exciting', 164),\n",
       "             ('choreography', 13),\n",
       "             ('taken', 71),\n",
       "             ('me', 222),\n",
       "             ('long', 364),\n",
       "             ('grips', 6),\n",
       "             ('cinematic', 177),\n",
       "             ('weakness', 6),\n",
       "             ('maybe', 97),\n",
       "             ('blessing', 5),\n",
       "             ('i', 962),\n",
       "             ('especially', 127),\n",
       "             ('music', 198),\n",
       "             ('bouncy', 2),\n",
       "             ('happy', 70),\n",
       "             ('infectious', 9),\n",
       "             ('sequence', 43),\n",
       "             ('stage', 43),\n",
       "             ('movement', 30),\n",
       "             ('wholly', 18),\n",
       "             ('appreciated', 12),\n",
       "             ('those', 364),\n",
       "             ('become', 119),\n",
       "             ('infectiously', 8),\n",
       "             ('rare', 109),\n",
       "             ('improves', 10),\n",
       "             ('dancing', 23),\n",
       "             ('better', 445),\n",
       "             ('grittier', 4),\n",
       "             ('sexier', 3),\n",
       "             ('energetic', 22),\n",
       "             ('diverse', 9),\n",
       "             ('delightful', 63),\n",
       "             ('learning', 9),\n",
       "             ('miles', 12),\n",
       "             ('teller', 8),\n",
       "             ('steals', 15),\n",
       "             ('fantastic', 53),\n",
       "             ('dancers', 2),\n",
       "             ('kenny', 1),\n",
       "             ('julianne', 1),\n",
       "             ('risibly', 1),\n",
       "             ('fashioned', 136),\n",
       "             ('pleaserweirdly', 1),\n",
       "             ('constrained', 1),\n",
       "             ('determination', 12),\n",
       "             ('ape', 8),\n",
       "             ('earnestlyso', 1),\n",
       "             ('clean', 20),\n",
       "             ('wholesome', 12),\n",
       "             ('mtv', 4),\n",
       "             ('imprimatur', 2),\n",
       "             ('attached', 24),\n",
       "             ('seems', 204),\n",
       "             ('joke', 50),\n",
       "             ('completely', 95),\n",
       "             ('harmless', 11),\n",
       "             ('quite', 260),\n",
       "             ('any', 427),\n",
       "             ('pop', 82),\n",
       "             ('song', 27),\n",
       "             ('got', 107),\n",
       "             ('plays', 175),\n",
       "             ('slightly', 50),\n",
       "             ('degraded', 2),\n",
       "             ('version', 183),\n",
       "             ('dialogue', 205),\n",
       "             ('lamer', 1),\n",
       "             ('poorer', 1),\n",
       "             ('dont', 30),\n",
       "             ('hate', 21),\n",
       "             ('expression', 16),\n",
       "             ('thumper', 1),\n",
       "             ('rollicking', 10),\n",
       "             ('tweaking', 3),\n",
       "             ('brings', 104),\n",
       "             ('things', 206),\n",
       "             ('date', 120),\n",
       "             ('including', 55),\n",
       "             ('few', 237),\n",
       "             ('salutes', 5),\n",
       "             ('rens', 1),\n",
       "             ('bright', 35),\n",
       "             ('yellow', 7),\n",
       "             ('vw', 1),\n",
       "             ('beetle', 1),\n",
       "             ('ariels', 1),\n",
       "             ('red', 47),\n",
       "             ('leather', 4),\n",
       "             ('cowboy', 11),\n",
       "             ('boots', 18),\n",
       "             ('looking', 153),\n",
       "             ('athletic', 4),\n",
       "             ('kick', 66),\n",
       "             ('heels', 7),\n",
       "             ('gusto', 11),\n",
       "             ('choreographer', 3),\n",
       "             ('jamal', 1),\n",
       "             ('sims', 1),\n",
       "             ('melds', 4),\n",
       "             ('line', 127),\n",
       "             ('hip', 50),\n",
       "             ('hop', 31),\n",
       "             ('always', 135),\n",
       "             ('harking', 1),\n",
       "             ('back', 234),\n",
       "             ('affectionately', 4),\n",
       "             ('iconic', 14),\n",
       "             ('moments', 262),\n",
       "             ('earlier', 46),\n",
       "             ('craig', 8),\n",
       "             ('directs', 30),\n",
       "             ('electricity', 2),\n",
       "             ('emanating', 1),\n",
       "             ('fingertips', 1),\n",
       "             ('might', 332),\n",
       "             ('point', 162),\n",
       "             ('ignite', 10),\n",
       "             ('into', 918),\n",
       "             ('fiery', 7),\n",
       "             ('ball', 13),\n",
       "             ('sex', 208),\n",
       "             ('talk', 49),\n",
       "             ('when', 528),\n",
       "             ('having', 80),\n",
       "             ('fun', 488),\n",
       "             ('use', 65),\n",
       "             ('songs', 42),\n",
       "             ('1984', 7),\n",
       "             ('too', 605),\n",
       "             ('dragging', 11),\n",
       "             ('pulse', 20),\n",
       "             ('barely', 43),\n",
       "             ('considering', 30),\n",
       "             ('essentially', 34),\n",
       "             ('carbon', 2),\n",
       "             ('copy', 4),\n",
       "             ('less', 242),\n",
       "             ('chemistry', 86),\n",
       "             ('leading', 70),\n",
       "             ('actor', 151),\n",
       "             ('who', 973),\n",
       "             ('sounded', 5),\n",
       "             ('mark', 79),\n",
       "             ('whalberg', 1),\n",
       "             ('foot', 29),\n",
       "             ('tapping', 5),\n",
       "             ('sexy', 49),\n",
       "             ('romance', 141),\n",
       "             ('changes', 17),\n",
       "             ('mainstays', 1),\n",
       "             ('high', 297),\n",
       "             ('energy', 109),\n",
       "             ('classic', 282),\n",
       "             ('successfully', 16),\n",
       "             ('remixes', 1),\n",
       "             ('cult', 41),\n",
       "             ('flicks', 48),\n",
       "             ('start', 80),\n",
       "             ('finish', 33),\n",
       "             ('exudes', 10),\n",
       "             ('forebearers', 1),\n",
       "             ('semi', 14),\n",
       "             ('awkward', 25),\n",
       "             ('clunky', 17),\n",
       "             ('obedience', 2),\n",
       "             ('formula', 91),\n",
       "             ('borders', 6),\n",
       "             ('clich', 27),\n",
       "             ('somehow', 41),\n",
       "             ('works', 234),\n",
       "             ('previously', 13),\n",
       "             ('intensity', 27),\n",
       "             ('spin', 46),\n",
       "             ('hustle', 24),\n",
       "             ('flow', 12),\n",
       "             ('black', 253),\n",
       "             ('snake', 1),\n",
       "             ('moan', 1),\n",
       "             ('displays', 20),\n",
       "             ('coolest', 9),\n",
       "             ('smartly', 24),\n",
       "             ('unties', 1),\n",
       "             ('1980s', 15),\n",
       "             ('moorings', 1),\n",
       "             ('puts', 57),\n",
       "             ('enough', 459),\n",
       "             ('smarts', 17),\n",
       "             ('sweat', 7),\n",
       "             ('swagger', 6),\n",
       "             ('steps', 22),\n",
       "             ('help', 97),\n",
       "             ('move', 32),\n",
       "             ('your', 381),\n",
       "             ('feet', 14),\n",
       "             ('hum', 4),\n",
       "             ('along', 157),\n",
       "             ('hewing', 2),\n",
       "             ('closely', 11),\n",
       "             ('footlooses', 1),\n",
       "             ('themes', 69),\n",
       "             ('throws', 19),\n",
       "             ('standard', 53),\n",
       "             ('school', 124),\n",
       "             ('notion', 22),\n",
       "             ('teenage', 57),\n",
       "             ('caste', 3),\n",
       "             ('system', 21),\n",
       "             ('window', 10),\n",
       "             ('true', 310),\n",
       "             ('venture', 4),\n",
       "             ('theater', 50),\n",
       "             ('showing', 41),\n",
       "             ('screenplay', 92),\n",
       "             ('detriment', 5),\n",
       "             ('enjoyment', 10),\n",
       "             ('talking', 41),\n",
       "             ('would', 524),\n",
       "             ('fancy', 11),\n",
       "             ('free', 60),\n",
       "             ('environment', 10),\n",
       "             ('after', 266),\n",
       "             ('sophomore', 8),\n",
       "             ('jinx', 3),\n",
       "             ('ground', 57),\n",
       "             ('retro', 18),\n",
       "             ('cool', 93),\n",
       "             ('musical', 72),\n",
       "             ('parents', 102),\n",
       "             ('grandparents', 2),\n",
       "             ('due', 39),\n",
       "             ('unabashedly', 9),\n",
       "             ('corny', 10),\n",
       "             ('nature', 109),\n",
       "             ('walking', 24),\n",
       "             ('shoes', 9),\n",
       "             ('sassier', 1),\n",
       "             ('spring', 7),\n",
       "             ('step', 67),\n",
       "             ('certain', 59),\n",
       "             ('final', 144),\n",
       "             ('why', 131),\n",
       "             ('all', 1252),\n",
       "             ('glue', 2),\n",
       "             ('subtly', 20),\n",
       "             ('holding', 12),\n",
       "             ('everything', 132),\n",
       "             ('together', 155),\n",
       "             ('intelligent', 108),\n",
       "             ('atmospheric', 14),\n",
       "             ('direction', 165),\n",
       "             ('known', 56),\n",
       "             ('superb', 86),\n",
       "             ('handling', 9),\n",
       "             ('may', 500),\n",
       "             ('adhere', 1),\n",
       "             ('immensely', 13),\n",
       "             ('ride', 91),\n",
       "             ('boosted', 2),\n",
       "             ('pitch', 62),\n",
       "             ('perfect', 193),\n",
       "             ('instincts', 11),\n",
       "             ('nothing', 185),\n",
       "             ('challenges', 17),\n",
       "             ('introduce', 7),\n",
       "             ('generation', 52),\n",
       "             ('heart', 318),\n",
       "             ('there', 646),\n",
       "             ('wrong', 96),\n",
       "             ('frank', 71),\n",
       "             ('funny', 684),\n",
       "             ('tender', 46),\n",
       "             ('asks', 4),\n",
       "             ('receives', 2),\n",
       "             ('seen', 288),\n",
       "             ('sessions', 15),\n",
       "             ('pleasant', 36),\n",
       "             ('shock', 39),\n",
       "             ('touching', 93),\n",
       "             ('profoundly', 15),\n",
       "             ('positive', 13),\n",
       "             ('equates', 2),\n",
       "             ('intimacy', 15),\n",
       "             ('tenderness', 10),\n",
       "             ('emotional', 227),\n",
       "             ('connection', 15),\n",
       "             ('instead', 134),\n",
       "             ('competition', 28),\n",
       "             ('conquest', 2),\n",
       "             ('john', 269),\n",
       "             ('hawkes', 15),\n",
       "             ('awards', 20),\n",
       "             ('invented', 6),\n",
       "             ('exhilarating', 61),\n",
       "             ('gift', 32),\n",
       "             ('beautifully', 141),\n",
       "             ('typical', 46),\n",
       "             ('biopic', 53),\n",
       "             ('since', 216),\n",
       "             ('finds', 85),\n",
       "             ('instilling', 1),\n",
       "             ('humor', 253),\n",
       "             ('charm', 117),\n",
       "             ('delicate', 24),\n",
       "             ('often', 240),\n",
       "             ('tragic', 34),\n",
       "             ('delivers', 186),\n",
       "             ('perhaps', 119),\n",
       "             ('career', 129),\n",
       "             ('helen', 39),\n",
       "             ('hunt', 22),\n",
       "             ('fearless', 17),\n",
       "             ('charming', 118),\n",
       "             ('heartfelt', 45),\n",
       "             ('journey', 55),\n",
       "             ('leave', 107),\n",
       "             ('spectrum', 7),\n",
       "             ('overwhelmed', 5),\n",
       "             ('hearts', 25),\n",
       "             ('generate', 12),\n",
       "             ('endearing', 32),\n",
       "             ('turning', 39),\n",
       "             ('couple', 81),\n",
       "             ('virtuoso', 12),\n",
       "             ('deserving', 7),\n",
       "             ('serious', 82),\n",
       "             ('consideration', 7),\n",
       "             ('season', 48),\n",
       "             ('honest', 52),\n",
       "             ('assured', 26),\n",
       "             ('extremely', 62),\n",
       "             ('celebrates', 22),\n",
       "             ('sexuality', 19),\n",
       "             ('ben', 94),\n",
       "             ('lewins', 4),\n",
       "             ('given', 80),\n",
       "             ('biggest', 33),\n",
       "             ('juiciest', 1),\n",
       "             ('role', 151),\n",
       "             ('pulls', 29),\n",
       "             ('grace', 64),\n",
       "             ('emotionally', 96),\n",
       "             ('charged', 16),\n",
       "             ('smart', 211),\n",
       "             ('thatll', 9),\n",
       "             ('tearing', 2),\n",
       "             ('equal', 36),\n",
       "             ('doses', 4),\n",
       "             ('laughter', 21),\n",
       "             ('intentioned', 7),\n",
       "             ('examination', 20),\n",
       "             ('disabled', 6),\n",
       "             ('exploring', 20),\n",
       "             ('sexual', 72),\n",
       "             ('identity', 32),\n",
       "             ('needs', 89),\n",
       "             ('intentions', 27),\n",
       "             ('sell', 12),\n",
       "             ('truly', 112),\n",
       "             ('affirming', 17),\n",
       "             ('particular', 44),\n",
       "             ('lovely', 65),\n",
       "             ('poignant', 75),\n",
       "             ('earns', 23),\n",
       "             ('academy', 20),\n",
       "             ('doesnt', 14),\n",
       "             ('aided', 13),\n",
       "             ('committed', 17),\n",
       "             ('ready', 32),\n",
       "             ('transforms', 15),\n",
       "             ('taboo', 5),\n",
       "             ('matter', 117),\n",
       "             ('humorous', 22),\n",
       "             ('humane', 21),\n",
       "             ('uncomplicated', 3),\n",
       "             ('first', 590),\n",
       "             ('explore', 27),\n",
       "             ('sensitive', 28),\n",
       "             ('lewin', 1),\n",
       "             ('empathetic', 8),\n",
       "             ('condescend', 3),\n",
       "             ('among', 86),\n",
       "             ('brave', 18),\n",
       "             ('spends', 15),\n",
       "             ('entirely', 63),\n",
       "             ('naked', 18),\n",
       "             ('her', 428),\n",
       "             ('brilliant', 127),\n",
       "             ('massively', 3),\n",
       "             ('uplifting', 35),\n",
       "             ('becomes', 79),\n",
       "             ('portrayal', 35),\n",
       "             ('form', 120),\n",
       "             ('kindness', 8),\n",
       "             ('human', 289),\n",
       "             ('contact', 10),\n",
       "             ('adult', 73),\n",
       "             ('upends', 3),\n",
       "             ('conventions', 24),\n",
       "             ('comedy', 1009),\n",
       "             ('investing', 2),\n",
       "             ('them', 318),\n",
       "             ('solemn', 5),\n",
       "             ('purpose', 23),\n",
       "             ('fortunately', 15),\n",
       "             ('turns', 192),\n",
       "             ('pretty', 190),\n",
       "             ('itself', 183),\n",
       "             ('difficult', 57),\n",
       "             ('uncomfortable', 16),\n",
       "             ('honesty', 20),\n",
       "             ('ultimately', 146),\n",
       "             ('writer', 319),\n",
       "             ('slight', 30),\n",
       "             ('pat', 10),\n",
       "             ('wildly', 43),\n",
       "             ('overhyped', 1),\n",
       "             ('festival', 25),\n",
       "             ('showings', 1),\n",
       "             ('religion', 27),\n",
       "             ('poetry', 22),\n",
       "             ('prepare', 4),\n",
       "             ('emotions', 56),\n",
       "             ('shared', 16),\n",
       "             ('unique', 85),\n",
       "             ('unforgettable', 27),\n",
       "             ('triumph', 54),\n",
       "             ('over', 439),\n",
       "             ('disability', 6),\n",
       "             ('rather', 173),\n",
       "             ('depressing', 18),\n",
       "             ('saga', 74),\n",
       "             ('dying', 8),\n",
       "             ('once', 194),\n",
       "             ('downright', 24),\n",
       "             ('cuddly', 5),\n",
       "             ('deals', 15),\n",
       "             ('seldom', 16),\n",
       "             ('visited', 3),\n",
       "             ('lives', 108),\n",
       "             ('people', 308),\n",
       "             ('disabilities', 2),\n",
       "             ('belongs', 14),\n",
       "             ('disappears', 1),\n",
       "             ('obrien', 4),\n",
       "             ('delivering', 43),\n",
       "             ('stunning', 86),\n",
       "             ('illuminates', 4),\n",
       "             ('means', 62),\n",
       "             ('whole', 136),\n",
       "             ('person', 38),\n",
       "             ('hook', 15),\n",
       "             ('gives', 249),\n",
       "             ('soul', 111),\n",
       "             ('frightening', 21),\n",
       "             ('rustic', 1),\n",
       "             ('winters', 2),\n",
       "             ('bone', 18),\n",
       "             ('martha', 8),\n",
       "             ('marcy', 2),\n",
       "             ('marlene', 2),\n",
       "             ('witty', 94),\n",
       "             ('warm', 64),\n",
       "             ('hearted', 41),\n",
       "             ('frequently', 50),\n",
       "             ('laugh', 111),\n",
       "             ('loud', 65),\n",
       "             ('pair', 30),\n",
       "             ('gem', 28),\n",
       "             ('largely', 49),\n",
       "             ('dynamic', 17),\n",
       "             ('incredibly', 34),\n",
       "             ('key', 58),\n",
       "             ('funnier', 31),\n",
       "             ('probably', 129),\n",
       "             ('expecting', 18),\n",
       "             ('highlighted', 5),\n",
       "             ('caliber', 21),\n",
       "             ('leads', 65),\n",
       "             ('neither', 63),\n",
       "             ('issue', 51),\n",
       "             ('pushing', 24),\n",
       "             ('crude', 24),\n",
       "             ('american', 261),\n",
       "             ('polio', 1),\n",
       "             ('sweet', 159),\n",
       "             ('winning', 104),\n",
       "             ('minus', 7),\n",
       "             ('fingers', 5),\n",
       "             ('down', 260),\n",
       "             ('throat', 11),\n",
       "             ('exploration', 26),\n",
       "             ('compensates', 7),\n",
       "             ('minor', 42),\n",
       "             ('structural', 2),\n",
       "             ('misstep', 3),\n",
       "             ('acute', 11),\n",
       "             ('ear', 18),\n",
       "             ('tone', 99),\n",
       "             ('stellar', 16),\n",
       "             ('throughout', 64),\n",
       "             ('inspired', 101),\n",
       "             ('late', 69),\n",
       "             ('poet', 4),\n",
       "             ('journalist', 4),\n",
       "             ('relationship', 64),\n",
       "             ('between', 390),\n",
       "             ('physical', 37),\n",
       "             ('another', 299),\n",
       "             ('weepy', 3),\n",
       "             ('overcoming', 2),\n",
       "             ('odds', 25),\n",
       "             ('my', 157),\n",
       "             ('left', 80),\n",
       "             ('different', 96),\n",
       "             ('appendage', 1),\n",
       "             ('brazenly', 3),\n",
       "             ('reacting', 3),\n",
       "             ('flustering', 1),\n",
       "             ('away', 157),\n",
       "             ('nudity', 9),\n",
       "             ('minded', 27),\n",
       "             ('tv', 87),\n",
       "             ('unusually', 19),\n",
       "             ('actors', 191),\n",
       "             ('inevitable', 21),\n",
       "             ('comparison', 13),\n",
       "             ('terms', 67),\n",
       "             ('challenging', 36),\n",
       "             ('breaking', 69),\n",
       "             ('hollywood', 247),\n",
       "             ('taboos', 5),\n",
       "             ('superbly', 58),\n",
       "             ('acted', 159),\n",
       "             ('bracing', 12),\n",
       "             ('recognize', 11),\n",
       "             ('severe', 6),\n",
       "             ('magnificent', 28),\n",
       "             ('tale', 347),\n",
       "             ('mans', 59),\n",
       "             ('awaking', 1),\n",
       "             ('genuine', 68),\n",
       "             ('pleaser', 30),\n",
       "             ('voice', 57),\n",
       "             ('marginalized', 1),\n",
       "             ('built', 36),\n",
       "             ('around', 178),\n",
       "             ('powerfully', 24),\n",
       "             ('evil', 59),\n",
       "             ('carries', 31),\n",
       "             ('moving', 184),\n",
       "             ('worthy', 113),\n",
       "             ('invests', 9),\n",
       "             ('nullifies', 1),\n",
       "             ('potential', 43),\n",
       "             ('adults', 103),\n",
       "             ('older', 33),\n",
       "             ('matches', 4),\n",
       "             ('portrays', 13),\n",
       "             ('gabrielle', 6),\n",
       "             ('claustrophobic', 12),\n",
       "             ('stifling', 4),\n",
       "             ('crusty', 2),\n",
       "             ('saved', 18),\n",
       "             ...])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedWord_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words in extracted Reviews:-> 25588\n"
     ]
    }
   ],
   "source": [
    "print('No of unique words in extracted Reviews:->',len(extractedWord_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarytokenizer=Tokenizer(char_level=False)\n",
    "summarytokenizer.fit_on_texts(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryWord_counts=summarytokenizer.word_counts\n",
    "summaryWord_index=summarytokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryWord_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4866"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summarytokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially vocab size for modified summary is:-> 4866\n",
      "After adding PAD to vocab, vocab size for modified summary is:-> 4867\n",
      "After adding SOS to vocab, vocab size for modified summary is:-> 4868\n",
      "After adding EOS to vocab, vocab size for modified summary is:-> 4869\n"
     ]
    }
   ],
   "source": [
    "modiefiedSummaryWord_index=summarytokenizer.word_index\n",
    "print('Initially vocab size for modified summary is:->',len(modiefiedSummaryWord_index))\n",
    "modiefiedSummaryWord_index['PAD']=0\n",
    "print('After adding PAD to vocab, vocab size for modified summary is:->',len(modiefiedSummaryWord_index))\n",
    "modiefiedSummaryWord_index['SOS']=len(summarytokenizer.word_counts)+1\n",
    "print('After adding SOS to vocab, vocab size for modified summary is:->',len(modiefiedSummaryWord_index))\n",
    "modiefiedSummaryWord_index['EOS']=len(summarytokenizer.word_counts)+2\n",
    "print('After adding EOS to vocab, vocab size for modified summary is:->',len(modiefiedSummaryWord_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 373)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryWord_index['battle'],modiefiedSummaryWord_index['battle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings = dict()\n",
    "filename = 'glove.6B/glove.6B.100d.txt'\n",
    "f = open(filename)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words in summaries:-> 4866\n"
     ]
    }
   ],
   "source": [
    "print('No of unique words in summaries:->',len(summarytokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReviewsVocabSize:-> 25589\n",
      "SummaryVocabSize:-> 4870\n",
      "ModifiedVocabSize:-> 4870\n"
     ]
    }
   ],
   "source": [
    "ReviewsVocabSize=len(extractedtokenizer.word_index)+1\n",
    "SummaryVocabSize=len(summarytokenizer.word_index)+1#actually it is 11279\n",
    "ModifiedVocabSize=len(modiefiedSummaryWord_index)+1\n",
    "print('ReviewsVocabSize:->',ReviewsVocabSize)\n",
    "print('SummaryVocabSize:->',SummaryVocabSize)\n",
    "print('ModifiedVocabSize:->',ModifiedVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4867"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryWord_index['SOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4867"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modiefiedSummaryWord_index['SOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 14\n",
      "Percent of words that are missing from vocabulary: 0.05%\n",
      "check:-> 20950\n",
      "most_good_repeating_words 2607\n",
      "Bad words 2017\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing in Glove, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "thresholdR = 20\n",
    "thresholdS = 0\n",
    "check=0#words that repeats less than 20 times but are in embedding_index \n",
    "most_good_repeating_words=0\n",
    "badWords=0# words which repeats less than 20 times and also not in embedding_index\n",
    "for word, count in extractedWord_counts.items():\n",
    "    if count <thresholdR:\n",
    "        if word in embeddings:\n",
    "            check+=1\n",
    "    if count <thresholdR:\n",
    "        if word not in embeddings:\n",
    "            badWords+=1\n",
    "            \n",
    "    if count >=thresholdR:\n",
    "        if word in embeddings:\n",
    "            most_good_repeating_words+=1\n",
    "    if count > thresholdR:\n",
    "        if word not in embeddings:\n",
    "            #print(word)\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(extractedWord_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))\n",
    "print('check:->',check)\n",
    "print('most_good_repeating_words',most_good_repeating_words)\n",
    "print('Bad words',badWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'woods': 5451,\n",
       " 'hanging': 4211,\n",
       " 'woody': 602,\n",
       " 'comically': 10673,\n",
       " 'scold': 12076,\n",
       " 'originality': 1810,\n",
       " 'appropriation': 12400,\n",
       " 'bringing': 1360,\n",
       " 'wooded': 16461,\n",
       " 'liaisons': 19556,\n",
       " 'grueling': 5793,\n",
       " 'wooden': 7370,\n",
       " 'circuitry': 20058,\n",
       " 'crotch': 14366,\n",
       " 'stereotypical': 25102,\n",
       " 'gwenns': 19422,\n",
       " 'insular': 20253,\n",
       " 'splendiferous': 17601,\n",
       " 'precocity': 16462,\n",
       " 'sustaining': 12064,\n",
       " 'consenting': 22402,\n",
       " 'scraped': 16876,\n",
       " 'inanimate': 17468,\n",
       " 'errors': 13436,\n",
       " 'cooking': 14568,\n",
       " 'videodrome': 12964,\n",
       " 'succumb': 7772,\n",
       " 'shocks': 4237,\n",
       " 'brainwashed': 24064,\n",
       " '47': 18380,\n",
       " 'perfunctorily': 18615,\n",
       " 'china': 3559,\n",
       " 'natured': 2218,\n",
       " 'kids': 232,\n",
       " 'uplifting': 1482,\n",
       " 'climbed': 19524,\n",
       " 'controversy': 4161,\n",
       " 'natures': 14968,\n",
       " 'spotty': 8787,\n",
       " 'golden': 1890,\n",
       " 'projection': 25391,\n",
       " 'outraging': 15989,\n",
       " 'darkos': 22207,\n",
       " 'fantasia2000': 13123,\n",
       " 'ohorten': 7993,\n",
       " 'dna': 8000,\n",
       " 'catchy': 5921,\n",
       " 'insecurity': 8940,\n",
       " 'cannibal': 11624,\n",
       " 'prospector': 23141,\n",
       " 'music': 261,\n",
       " 'therefore': 5973,\n",
       " 'mystic': 24324,\n",
       " 'marber': 9324,\n",
       " 'gerrolds': 12910,\n",
       " 'yahoo': 23517,\n",
       " 'exuberantly': 19423,\n",
       " 'boorman': 15226,\n",
       " 'serially': 21860,\n",
       " 'circumstances': 3015,\n",
       " 'morally': 3471,\n",
       " 'locked': 23731,\n",
       " 'locker': 7913,\n",
       " 'sandier': 17637,\n",
       " 'expressively': 16532,\n",
       " 'wang': 22331,\n",
       " 'wand': 23540,\n",
       " 'unjust': 21695,\n",
       " 'titanium': 20909,\n",
       " 'want': 280,\n",
       " 'absolute': 2398,\n",
       " 'travel': 2858,\n",
       " 'copious': 10219,\n",
       " 'dangerfield': 16507,\n",
       " 'palaces': 19237,\n",
       " 'cadence': 16670,\n",
       " 'barbra': 8961,\n",
       " 'assimilated': 18310,\n",
       " 'highsmith': 22146,\n",
       " 'dinosaurs': 11465,\n",
       " 'wrong': 543,\n",
       " 'cerebrally': 13478,\n",
       " 'colorfully': 18450,\n",
       " 'chandor': 4742,\n",
       " 'subplots': 5272,\n",
       " 'sickening': 10899,\n",
       " '18th': 6801,\n",
       " 'concoction': 8320,\n",
       " 'reiners': 11302,\n",
       " 'nonsensical': 4201,\n",
       " 'disengaging': 20320,\n",
       " 'snugly': 18508,\n",
       " 'welcomed': 13467,\n",
       " 'stoicism': 17331,\n",
       " '620': 20999,\n",
       " 'rewarded': 4208,\n",
       " 'welcomes': 18031,\n",
       " 'wickedly': 2380,\n",
       " 'fit': 1730,\n",
       " 'screaming': 2395,\n",
       " 'gridiron': 3557,\n",
       " 'striking': 2290,\n",
       " 'fic': 22237,\n",
       " 'wisecrackers': 15654,\n",
       " 'wales': 20246,\n",
       " 'fin': 16955,\n",
       " 'zucker': 8073,\n",
       " 'sinuously': 19258,\n",
       " 'paterfamilias': 23377,\n",
       " 'songwriter': 14727,\n",
       " 'twits': 16201,\n",
       " 'recollections': 14772,\n",
       " 'effects': 150,\n",
       " 'comprised': 21134,\n",
       " 'undeveloped': 23812,\n",
       " 'glutted': 24004,\n",
       " 'knackfor': 18744,\n",
       " 'barton': 14516,\n",
       " 'arrow': 12487,\n",
       " 'ingrid': 7979,\n",
       " 'telescope': 16751,\n",
       " 'tantalisingly': 21141,\n",
       " 'tricked': 24406,\n",
       " 'esoterica': 19192,\n",
       " 'oprah': 12219,\n",
       " 'scrumptiously': 22478,\n",
       " 'encourage': 9364,\n",
       " 'adapt': 8901,\n",
       " 'abbott': 11722,\n",
       " 'underfoot': 15396,\n",
       " 'pumpkins': 23882,\n",
       " 'gameplay': 16419,\n",
       " 'universally': 13608,\n",
       " 'competes': 25545,\n",
       " 'sickeningly': 23605,\n",
       " 'disturbed': 10701,\n",
       " 'competed': 19962,\n",
       " 'portmanteau': 16004,\n",
       " 'loudness': 15533,\n",
       " 'olds': 3140,\n",
       " 'service': 3459,\n",
       " 'forrester': 4735,\n",
       " 'needed': 1301,\n",
       " 'master': 794,\n",
       " 'critter': 13575,\n",
       " 'genesis': 21941,\n",
       " 'rewards': 6457,\n",
       " 'enthrall': 12666,\n",
       " 'asylums': 18860,\n",
       " 'mutilated': 21269,\n",
       " 'positively': 4614,\n",
       " 'rewardingly': 14703,\n",
       " 'idle': 24014,\n",
       " 'sheen': 6925,\n",
       " 'feeling': 433,\n",
       " 'longs': 9172,\n",
       " 'spectrum': 5703,\n",
       " 'longo': 16124,\n",
       " 'brawny': 8978,\n",
       " 'protags': 16913,\n",
       " 'dozen': 2454,\n",
       " 'affairs': 7290,\n",
       " 'wholesome': 3860,\n",
       " 'beltway': 13768,\n",
       " 'toothed': 13997,\n",
       " 'adenikes': 17197,\n",
       " 'committing': 13135,\n",
       " 'jammed': 10640,\n",
       " 'frothiest': 23906,\n",
       " 'limitless': 10958,\n",
       " 'diminishing': 8551,\n",
       " 'cinematic': 293,\n",
       " 'resonates': 8325,\n",
       " 'disjointed': 5948,\n",
       " 'tensely': 18632,\n",
       " 'mouth': 3473,\n",
       " 'reverence': 11107,\n",
       " 'bradford': 17198,\n",
       " 'singer': 2119,\n",
       " 'tech': 2672,\n",
       " 'scream': 4144,\n",
       " 'saying': 1761,\n",
       " 'padded': 7852,\n",
       " 'hillbillies': 22874,\n",
       " 'chandors': 6982,\n",
       " 'tempted': 14831,\n",
       " 'cheaply': 22424,\n",
       " 'rico': 20590,\n",
       " 'assaultive': 13643,\n",
       " 'bliss': 4008,\n",
       " 'rick': 5954,\n",
       " 'rich': 519,\n",
       " 'rice': 21391,\n",
       " 'rica': 11879,\n",
       " 'plate': 10724,\n",
       " 'remotest': 18578,\n",
       " 'altogether': 4266,\n",
       " 'affronted': 25392,\n",
       " 'zestfully': 23365,\n",
       " 'droning': 25477,\n",
       " 'rectal': 20785,\n",
       " 'nicely': 1699,\n",
       " 'patch': 11475,\n",
       " 'programmatic': 18073,\n",
       " 'sensitivity': 2574,\n",
       " 'pinon': 11651,\n",
       " 'playfulness': 9086,\n",
       " 'lots': 1078,\n",
       " 'ira': 11384,\n",
       " 'imprimatur': 11688,\n",
       " 'discipline': 8176,\n",
       " 'extend': 9042,\n",
       " 'nature': 474,\n",
       " 'extent': 6117,\n",
       " 'tyranny': 18388,\n",
       " 'veer': 14991,\n",
       " 'voyeuristic': 11870,\n",
       " 'heating': 15445,\n",
       " 'fearlessly': 5343,\n",
       " 'eradicate': 19271,\n",
       " 'obscenity': 19155,\n",
       " 'maclaine': 12303,\n",
       " 'tmnt': 6912,\n",
       " 'amenbars': 11352,\n",
       " 'wearying': 11985,\n",
       " 'unhealed': 23774,\n",
       " 'minuses': 18981,\n",
       " 'blonde': 13868,\n",
       " 'smackdown': 15149,\n",
       " 'underdone': 20149,\n",
       " 'milquetoast': 19525,\n",
       " 'union': 3355,\n",
       " 'bothers': 15159,\n",
       " 'much': 59,\n",
       " 'fry': 20562,\n",
       " 'tallest': 23864,\n",
       " 'toning': 19955,\n",
       " 'obese': 8622,\n",
       " 'retrospect': 7156,\n",
       " 'spit': 10549,\n",
       " 'arkin': 19121,\n",
       " 'davy': 8437,\n",
       " 'dave': 2603,\n",
       " 'doubts': 13776,\n",
       " 'spin': 1160,\n",
       " 'skilfully': 5879,\n",
       " 'mesmerising': 5407,\n",
       " 'employ': 14825,\n",
       " 'misconstrued': 23368,\n",
       " 'nolans': 5929,\n",
       " 'k': 9777,\n",
       " 'majid': 11537,\n",
       " 'ditching': 18864,\n",
       " 'kohn': 16196,\n",
       " 'conditioned': 9865,\n",
       " 'eighteen': 11782,\n",
       " 'haplessly': 15319,\n",
       " 'oxymoron': 20681,\n",
       " 'breakfast': 24552,\n",
       " 'hone': 12005,\n",
       " 'hong': 2433,\n",
       " 'inventively': 8576,\n",
       " 'whimsicalities': 22203,\n",
       " 'jolting': 10414,\n",
       " 'democracies': 20047,\n",
       " 'split': 8963,\n",
       " 'visitations': 18930,\n",
       " 'principals': 19459,\n",
       " 'kasdan': 4382,\n",
       " 'boiled': 6738,\n",
       " 'effortlessly': 3348,\n",
       " 'marches': 11929,\n",
       " 'boiler': 3566,\n",
       " 'sissako': 7032,\n",
       " 'featherweight': 23937,\n",
       " 'downwardly': 22630,\n",
       " 'mentors': 15726,\n",
       " 'academic': 8250,\n",
       " 'stillness': 9765,\n",
       " 'academia': 21622,\n",
       " 'goofing': 14357,\n",
       " 'corporate': 1740,\n",
       " 'gigolo': 22863,\n",
       " 'absurdities': 8546,\n",
       " 'appropriately': 3964,\n",
       " 'portrayed': 8419,\n",
       " 'lasso': 12278,\n",
       " 'blech': 22214,\n",
       " 'hal': 3007,\n",
       " 'ham': 4905,\n",
       " 'had': 225,\n",
       " 'hay': 17620,\n",
       " 'mcnamara': 24110,\n",
       " 'beloved': 1433,\n",
       " 'has': 28,\n",
       " 'hat': 4799,\n",
       " 'haw': 17654,\n",
       " 'unblinkingly': 22532,\n",
       " 'elders': 17706,\n",
       " 'survival': 1801,\n",
       " 'resined': 23519,\n",
       " 'unequivocally': 14227,\n",
       " 'otherworldly': 7783,\n",
       " 'shadow': 4315,\n",
       " 'ballhaus': 23621,\n",
       " 'tennants': 13306,\n",
       " 'alice': 4210,\n",
       " 'niels': 18015,\n",
       " 'misdemeanors': 13941,\n",
       " 'unabashedly': 4775,\n",
       " 'beneficial': 11805,\n",
       " 'crowd': 660,\n",
       " 'crowe': 693,\n",
       " 'czech': 7046,\n",
       " 'crown': 10231,\n",
       " 'topping': 22185,\n",
       " 'captive': 10949,\n",
       " 'hemsworth': 11586,\n",
       " 'bottom': 3272,\n",
       " 'chabert': 17119,\n",
       " 'inhuman': 25572,\n",
       " 'plucked': 24197,\n",
       " 'monogamy': 15835,\n",
       " 'goetze': 18091,\n",
       " 'starring': 912,\n",
       " 'stoker': 15216,\n",
       " 'stokes': 13171,\n",
       " 'oneness': 23630,\n",
       " 'creatio': 17161,\n",
       " 'marshall': 2732,\n",
       " 'honeymoon': 17536,\n",
       " 'beings': 2855,\n",
       " 'hallucinogenic': 21246,\n",
       " 'shoots': 5783,\n",
       " 'despised': 14184,\n",
       " 'appealingly': 16946,\n",
       " 'tame': 7823,\n",
       " 'grasping': 19799,\n",
       " 'coarser': 24165,\n",
       " 'greatness': 2164,\n",
       " 'grooms': 16491,\n",
       " 'spurting': 24428,\n",
       " 'eastwoods': 2177,\n",
       " 'musclebound': 16008,\n",
       " 'megabudget': 17521,\n",
       " 'congratulations': 11172,\n",
       " 'humbled': 21874,\n",
       " 'masquerading': 5085,\n",
       " 'arrays': 16674,\n",
       " 'eyres': 20644,\n",
       " 'complications': 7145,\n",
       " 'stouthearted': 23171,\n",
       " 'duet': 10657,\n",
       " 'dues': 14347,\n",
       " 'passenger': 24640,\n",
       " 'disgrace': 23332,\n",
       " 'barrymore': 2994,\n",
       " 'unnerve': 16560,\n",
       " 'decapitation': 18917,\n",
       " 'eventual': 11411,\n",
       " 'cambodia': 13111,\n",
       " 'role': 344,\n",
       " 'obliges': 16587,\n",
       " 'wordlessly': 23787,\n",
       " 'roll': 2196,\n",
       " 'palms': 9162,\n",
       " 'rightblu': 18449,\n",
       " 'transported': 8478,\n",
       " 'palme': 14586,\n",
       " 'comely': 17921,\n",
       " 'intent': 4501,\n",
       " 'smelling': 16438,\n",
       " 'variable': 11248,\n",
       " 'transporter': 10088,\n",
       " 'hawkes': 3227,\n",
       " 'explosions': 2729,\n",
       " 'loren': 14828,\n",
       " 'shootout': 12367,\n",
       " 'gown': 18509,\n",
       " 'childs': 6699,\n",
       " 'chain': 5215,\n",
       " 'whoever': 9850,\n",
       " 'bandits': 9467,\n",
       " 'foregrounds': 23600,\n",
       " 'chair': 8028,\n",
       " 'ballet': 11640,\n",
       " 'grapples': 12620,\n",
       " 'everdeens': 16595,\n",
       " 'macho': 5781,\n",
       " 'oversight': 17089,\n",
       " 'tenacious': 10675,\n",
       " 'paychecks': 20117,\n",
       " 'jerk': 24337,\n",
       " 'lard': 19343,\n",
       " 'choice': 1762,\n",
       " 'embark': 15905,\n",
       " 'gloomy': 13223,\n",
       " 'ghostbusters': 6287,\n",
       " 'stays': 3153,\n",
       " 'glooms': 21755,\n",
       " 'exact': 7180,\n",
       " 'minute': 652,\n",
       " 'cooks': 22745,\n",
       " 'wojtowicz': 8602,\n",
       " 'minnie': 8983,\n",
       " 'skewed': 7570,\n",
       " 'ealing': 20301,\n",
       " 'meadow': 23250,\n",
       " 'rogozhkins': 13724,\n",
       " 'hindered': 8147,\n",
       " 'heavyweight': 11256,\n",
       " 'shirts': 11204,\n",
       " 'incrementally': 21342,\n",
       " 'celebrated': 4299,\n",
       " '300': 3337,\n",
       " 'ground': 945,\n",
       " 'celebrates': 2271,\n",
       " 'unintentionally': 9291,\n",
       " 'climbs': 16548,\n",
       " 'honour': 11804,\n",
       " 'cullens': 23656,\n",
       " 'topicality': 11068,\n",
       " 'plucking': 22042,\n",
       " 'address': 7293,\n",
       " 'dwindling': 22556,\n",
       " 'chillers': 16464,\n",
       " 'accomplishes': 6220,\n",
       " 'dusty': 10477,\n",
       " 'impacted': 15596,\n",
       " 'streepall': 23870,\n",
       " 'cusack': 2413,\n",
       " 'timbuktu': 7031,\n",
       " 'accomplished': 1291,\n",
       " 'sprouted': 12692,\n",
       " 'sargents': 24619,\n",
       " 'inexpressive': 20394,\n",
       " 'boyles': 5163,\n",
       " 'cacophony': 14132,\n",
       " 'betraying': 17189,\n",
       " 'undergone': 21128,\n",
       " 'working': 601,\n",
       " 'vigour': 16953,\n",
       " 'opposed': 9088,\n",
       " 'alastair': 16085,\n",
       " 'fictionhorror': 21502,\n",
       " 'assimilation': 14771,\n",
       " 'thompson': 2476,\n",
       " 'riders': 16060,\n",
       " 'lowercase': 18839,\n",
       " 'originally': 6132,\n",
       " 'abortion': 5011,\n",
       " 'rozemas': 10193,\n",
       " 'following': 2065,\n",
       " 'admired': 9021,\n",
       " 'mirrors': 9015,\n",
       " 'parachute': 13975,\n",
       " 'locks': 15992,\n",
       " 'admires': 14849,\n",
       " 'doomy': 17148,\n",
       " 'vainly': 10844,\n",
       " 'ounce': 11214,\n",
       " 'telepath': 24012,\n",
       " 'custer': 19613,\n",
       " 'mythos': 8194,\n",
       " 'convincingly': 5181,\n",
       " 'fueled': 3332,\n",
       " 'brainless': 3607,\n",
       " 'egotistical': 21913,\n",
       " 'surfing': 3358,\n",
       " 'conscious': 2556,\n",
       " 'inhabiting': 11736,\n",
       " 'sandbox': 24528,\n",
       " 'forebears': 20205,\n",
       " 'wolves': 20851,\n",
       " 'pulled': 3464,\n",
       " 'impactful': 23940,\n",
       " 'years': 108,\n",
       " 'professors': 22937,\n",
       " 'structuring': 19321,\n",
       " 'episodes': 5970,\n",
       " 'disconnect': 10817,\n",
       " 'jia': 5109,\n",
       " 'milked': 13420,\n",
       " 'jim': 1085,\n",
       " 'troubles': 8672,\n",
       " 'jir': 15035,\n",
       " 'differentiating': 17124,\n",
       " 'spotlighting': 20410,\n",
       " 'suspension': 7279,\n",
       " 'troubled': 2455,\n",
       " 'modestly': 5331,\n",
       " 'inconsequence': 15810,\n",
       " 'indigenous': 25533,\n",
       " 'secularized': 16517,\n",
       " 'overpowering': 12873,\n",
       " 'workmanlike': 6681,\n",
       " 'gramms': 25251,\n",
       " 'bedevil': 15854,\n",
       " 'kaurismakis': 11031,\n",
       " 'seltzerfriedberg': 22700,\n",
       " 'instability': 13376,\n",
       " 'quarter': 5183,\n",
       " 'tightlipped': 15703,\n",
       " 'materializes': 17447,\n",
       " 'retrieve': 20771,\n",
       " 'bursting': 6010,\n",
       " 'frames': 8443,\n",
       " 'entering': 13797,\n",
       " 'disasters': 16755,\n",
       " 'troll': 9212,\n",
       " 'yojimbo': 19086,\n",
       " 'seriously': 1176,\n",
       " 'trauma': 10454,\n",
       " 'internet': 5894,\n",
       " 'calming': 15909,\n",
       " 'complicates': 16890,\n",
       " 'uptempo': 24878,\n",
       " 'ultraviolent': 13428,\n",
       " 'inwardly': 19162,\n",
       " 'complicated': 1786,\n",
       " 'crazier': 18442,\n",
       " 'benioff': 19737,\n",
       " 'modest': 1674,\n",
       " 'hamer': 7994,\n",
       " 'gordon': 3605,\n",
       " 'brokeback': 1803,\n",
       " 'neglect': 13626,\n",
       " 'dallaires': 14811,\n",
       " 'emotion': 1181,\n",
       " 'gunshot': 15943,\n",
       " 'saving': 2676,\n",
       " 'spoken': 4623,\n",
       " 'ong': 6920,\n",
       " 'reprisal': 23944,\n",
       " 'one': 27,\n",
       " 'plotless': 5815,\n",
       " 'exaggerations': 20778,\n",
       " 'stifler': 20032,\n",
       " 'oversimplifies': 23449,\n",
       " 'hausners': 25257,\n",
       " 'stifled': 24487,\n",
       " 'embalming': 15437,\n",
       " 'perdendo': 19968,\n",
       " 'oversimplified': 13557,\n",
       " 'lingering': 7236,\n",
       " 'unlikeliest': 17173,\n",
       " 'shawn': 7458,\n",
       " 'surges': 25352,\n",
       " 'snatch': 8350,\n",
       " 'disasterpieces': 15521,\n",
       " 'devito': 13383,\n",
       " 'ascendance': 23704,\n",
       " 'crossroads': 7932,\n",
       " 'rehab': 12661,\n",
       " 'wandering': 8125,\n",
       " 'illness': 3959,\n",
       " 'supersized': 18315,\n",
       " 'stylings': 7544,\n",
       " 'sumptuous': 2679,\n",
       " 'turned': 810,\n",
       " 'locations': 2992,\n",
       " 'jewels': 12071,\n",
       " 'nympho': 24404,\n",
       " 'tulio': 20314,\n",
       " 'turner': 14621,\n",
       " 'invite': 12388,\n",
       " 'zoe': 11268,\n",
       " 'fashionable': 9822,\n",
       " 'warriors': 5062,\n",
       " 'zoo': 13496,\n",
       " 'intends': 6486,\n",
       " 'hetherington': 7018,\n",
       " 'mayer': 20727,\n",
       " 'gibney': 20473,\n",
       " 'pimple': 16892,\n",
       " 'sprawls': 14236,\n",
       " 'imperatives': 18275,\n",
       " 'motivational': 21370,\n",
       " 'opposite': 4461,\n",
       " 'discerning': 10518,\n",
       " 'printed': 10940,\n",
       " 'knowingly': 18752,\n",
       " 'creditable': 24126,\n",
       " 'captivatingly': 17540,\n",
       " 'touchy': 11064,\n",
       " 'phil': 2971,\n",
       " 'roundelay': 16192,\n",
       " 'jittery': 14209,\n",
       " '1974s': 20498,\n",
       " 'feuds': 17294,\n",
       " 'goldwyns': 11014,\n",
       " 'friction': 14756,\n",
       " 'elah': 19607,\n",
       " 'inconsistent': 10519,\n",
       " 'heeding': 23182,\n",
       " 'aggressive': 8168,\n",
       " 'imagined': 4049,\n",
       " 'ensembler': 17024,\n",
       " 'ensembles': 13284,\n",
       " 'aimlessly': 11222,\n",
       " 'guarded': 11136,\n",
       " 'liberators': 22287,\n",
       " 'simplistic': 2248,\n",
       " 'klein': 24854,\n",
       " 'corseted': 15367,\n",
       " 'keenly': 6162,\n",
       " 'tortilla': 6215,\n",
       " 'vision': 701,\n",
       " 'enthralled': 11038,\n",
       " 'breslin': 20569,\n",
       " 'attenuated': 8240,\n",
       " 'audaciously': 7889,\n",
       " 'segmentsa': 16707,\n",
       " 'impressions': 15013,\n",
       " 'intoxicating': 5097,\n",
       " 'refreshes': 18035,\n",
       " 'retells': 24881,\n",
       " 'masturbatory': 24374,\n",
       " 'harvey': 3031,\n",
       " 'wests': 24048,\n",
       " 'refreshed': 13028,\n",
       " 'bibliophile': 18517,\n",
       " 'enjoys': 11097,\n",
       " 'aberdonian': 16787,\n",
       " 'moony': 23251,\n",
       " 'caan': 14412,\n",
       " 'planes': 8885,\n",
       " 'awards': 2494,\n",
       " 'menacing': 8952,\n",
       " 'concentrated': 12314,\n",
       " 'busting': 6412,\n",
       " 'matheson': 17732,\n",
       " 'millionaire': 11291,\n",
       " 'paring': 10951,\n",
       " 's': 2239,\n",
       " 'workplace': 6646,\n",
       " 'concentrates': 17972,\n",
       " 'flipper': 13850,\n",
       " 'grooming': 16737,\n",
       " 'fix': 3427,\n",
       " 'loveliest': 14783,\n",
       " 'rubberneckers': 18823,\n",
       " 'compels': 14402,\n",
       " 'buoyancy': 15542,\n",
       " 'collider': 13378,\n",
       " 'west': 1368,\n",
       " 'motives': 6066,\n",
       " 'wants': 670,\n",
       " 'formed': 6627,\n",
       " 'readings': 9535,\n",
       " 'photos': 6122,\n",
       " 'hashtag': 21044,\n",
       " 'abject': 20479,\n",
       " 'former': 1670,\n",
       " 'defeatist': 25462,\n",
       " 'pretence': 17956,\n",
       " 'squeezes': 11127,\n",
       " 'butchers': 24239,\n",
       " 'ives': 8032,\n",
       " 'revelatory': 3285,\n",
       " 'squeezed': 9841,\n",
       " 'situation': 1708,\n",
       " 'unsentimentally': 15500,\n",
       " 'unlikeable': 12560,\n",
       " 'reviled': 11845,\n",
       " 'dubious': 6514,\n",
       " 'engages': 7959,\n",
       " 'technology': 1263,\n",
       " 'ingrained': 25472,\n",
       " 'theorized': 16049,\n",
       " 'jamals': 19592,\n",
       " 'otto': 11028,\n",
       " 'bogglingly': 19548,\n",
       " 'defs': 20522,\n",
       " 'visually': 492,\n",
       " 'wires': 7930,\n",
       " 'edged': 5068,\n",
       " 'assigns': 16082,\n",
       " 'deft': 2117,\n",
       " 'defy': 6928,\n",
       " 'edges': 3883,\n",
       " 'wired': 8495,\n",
       " 'advertisement': 14689,\n",
       " 'tracking': 21493,\n",
       " 'droppingly': 8195,\n",
       " 'peculiarities': 23480,\n",
       " 'dimension': 3312,\n",
       " 'recycles': 5324,\n",
       " 'being': 179,\n",
       " 'bueller': 24200,\n",
       " 'recycled': 4426,\n",
       " 'mccorvey': 17805,\n",
       " 'rover': 3647,\n",
       " 'grounded': 2439,\n",
       " 'excuses': 12670,\n",
       " 'adroit': 21098,\n",
       " 'spoilsthats': 20601,\n",
       " 'overthrow': 17708,\n",
       " 'dicks': 16006,\n",
       " 'ogres': 17403,\n",
       " 'unerring': 20750,\n",
       " 'sportsmanship': 23624,\n",
       " 'sums': 9208,\n",
       " 'unveil': 20459,\n",
       " 'gestures': 8699,\n",
       " 'traffic': 4617,\n",
       " 'world': 98,\n",
       " 'sensational': 3922,\n",
       " 'unrepentant': 10156,\n",
       " 'hamiltons': 24118,\n",
       " 'superiority': 13889,\n",
       " 'goosey': 18893,\n",
       " 'satisfactory': 14054,\n",
       " 'gooses': 11071,\n",
       " 'tvs': 9746,\n",
       " 'diving': 13986,\n",
       " 'divine': 5623,\n",
       " 'butchery': 23334,\n",
       " 'francois': 13906,\n",
       " '911': 2347,\n",
       " 'restoring': 14097,\n",
       " 'disinformation': 17288,\n",
       " 'retains': 5428,\n",
       " 'leadership': 25474,\n",
       " 'thailand': 21332,\n",
       " 'prepared': 4196,\n",
       " 'exasperating': 19794,\n",
       " 'hopkins': 1510,\n",
       " 'disabilities': 11702,\n",
       " 'frights': 21162,\n",
       " 'niall': 14406,\n",
       " 'johnston': 4440,\n",
       " 'locklear': 8072,\n",
       " 'sensitively': 7560,\n",
       " 'shapely': 18077,\n",
       " 'antidote': 7285,\n",
       " 'ismerely': 21915,\n",
       " 'carey': 3549,\n",
       " 'ineffable': 12285,\n",
       " 'lively': 1377,\n",
       " 'bukater': 24652,\n",
       " 'gnashing': 17735,\n",
       " 'bubbly': 14758,\n",
       " 'confiture': 21106,\n",
       " 'gleam': 13371,\n",
       " 'glean': 13356,\n",
       " 'mindless': 2108,\n",
       " 'sealed': 15686,\n",
       " 'brazilian': 6913,\n",
       " 'bubble': 5320,\n",
       " 'prioritzes': 19892,\n",
       " 'wits': 6252,\n",
       " 'societal': 7281,\n",
       " 'with': 11,\n",
       " 'abused': 10332,\n",
       " 'pull': 1536,\n",
       " 'rush': 706,\n",
       " 'rage': 3057,\n",
       " 'assa': 18858,\n",
       " 'rags': 7171,\n",
       " 'dirty': 1419,\n",
       " 'abuses': 13114,\n",
       " 'russ': 17303,\n",
       " 'pulp': 1322,\n",
       " 'touchstone': 15553,\n",
       " 'gratuitous': 5348,\n",
       " 'watches': 10387,\n",
       " 'substantially': 24536,\n",
       " 'associating': 21014,\n",
       " 'watched': 2986,\n",
       " 'tremble': 8864,\n",
       " 'cream': 8400,\n",
       " 'moniker': 13130,\n",
       " 'ideally': 9400,\n",
       " 'sympathetically': 13357,\n",
       " 'introspection': 11239,\n",
       " 'unparalleled': 15078,\n",
       " 'friggin': 17244,\n",
       " 'puppy': 10318,\n",
       " 'waving': 9381,\n",
       " 'midget': 20770,\n",
       " 'fedoras': 21490,\n",
       " 'tricky': 4883,\n",
       " 'natalie': 3440,\n",
       " 'thora': 18356,\n",
       " 'omnibus': 9582,\n",
       " 'tricks': 3323,\n",
       " 'natalis': 11373,\n",
       " 'sci': 292,\n",
       " 'caused': 6125,\n",
       " 'beware': 24364,\n",
       " 'causes': 6929,\n",
       " 'flimsyish': 24653,\n",
       " 'cheney': 17144,\n",
       " 'adaps': 21255,\n",
       " 'nora': 16542,\n",
       " 'rants': 13969,\n",
       " 'countless': 6444,\n",
       " 'norm': 8397,\n",
       " 'megamind': 2632,\n",
       " 'sant': 4098,\n",
       " 'sans': 10733,\n",
       " 'shenanigans': 5959,\n",
       " 'sang': 8939,\n",
       " 'sand': 7669,\n",
       " 'sane': 16643,\n",
       " 'small': 412,\n",
       " 'sank': 10977,\n",
       " 'abbreviated': 15426,\n",
       " 'quicker': 25423,\n",
       " 'burwells': 18489,\n",
       " 'past': 367,\n",
       " 'displays': 2492,\n",
       " 'pass': 2565,\n",
       " 'investment': 6406,\n",
       " 'amarcord': 15633,\n",
       " 'clock': 8322,\n",
       " 'psychosomatic': 24296,\n",
       " 'gouts': 24491,\n",
       " 'conundrums': 24907,\n",
       " 'hewing': 11693,\n",
       " 'dwells': 9797,\n",
       " 'full': 212,\n",
       " 'hash': 23992,\n",
       " 'hasa': 20534,\n",
       " 'portrays': 3593,\n",
       " 'november': 16136,\n",
       " 'criminality': 12247,\n",
       " 'benignis': 24837,\n",
       " 'melancholic': 7587,\n",
       " 'melancholia': 20721,\n",
       " 'experience': 247,\n",
       " 'prior': 6709,\n",
       " 'newmans': 16964,\n",
       " 'skepticism': 8479,\n",
       " 'sizzlingly': 11024,\n",
       " 'welter': 16068,\n",
       " 'followed': 4872,\n",
       " 'retroactive': 24002,\n",
       " 'traumatized': 11731,\n",
       " 'follower': 15080,\n",
       " 'pobremente': 23634,\n",
       " 'cynics': 17944,\n",
       " 'silva': 11314,\n",
       " 'enliven': 9697,\n",
       " 'shanges': 5683,\n",
       " 'versailles': 5772,\n",
       " 'more': 32,\n",
       " 'door': 6323,\n",
       " 'initiated': 16669,\n",
       " 'tester': 20582,\n",
       " 'company': 892,\n",
       " 'tested': 4922,\n",
       " 'doob': 8751,\n",
       " 'doom': 4587,\n",
       " 'uncool': 10226,\n",
       " 'birdie': 20889,\n",
       " 'patriarch': 23062,\n",
       " 'learn': 2538,\n",
       " 'knocked': 4341,\n",
       " 'nervousness': 22404,\n",
       " 'bogs': 10365,\n",
       " 'memoirs': 17746,\n",
       " 'meaner': 10098,\n",
       " 'huge': 1850,\n",
       " 'respective': 10069,\n",
       " 'hugo': 12353,\n",
       " 'hugh': 2261,\n",
       " 'dismissed': 6427,\n",
       " 'hugs': 13231,\n",
       " 'sprinkle': 24396,\n",
       " 'lanky': 18426,\n",
       " 'intended': 1621,\n",
       " 'mendes': 2960,\n",
       " 'disgraced': 24494,\n",
       " 'brett': 8057,\n",
       " 'hackwork': 23932,\n",
       " 'mendacity': 22656,\n",
       " 'kusturica': 15897,\n",
       " 'malevolent': 11227,\n",
       " 'resemble': 14773,\n",
       " 'twisting': 6742,\n",
       " 'overcooked': 7657,\n",
       " 'signe': 19399,\n",
       " 'eveyone': 21667,\n",
       " 'peppy': 15765,\n",
       " 'installed': 22786,\n",
       " 'resorts': 20634,\n",
       " 'paper': 2195,\n",
       " 'scott': 681,\n",
       " 'signs': 1770,\n",
       " 'smiling': 3421,\n",
       " 'zhangkes': 8997,\n",
       " 'roots': 2397,\n",
       " 'bummed': 19857,\n",
       " 'tantalizingly': 8552,\n",
       " 'kneejerk': 20618,\n",
       " 'bummer': 11394,\n",
       " 'isaac': 9340,\n",
       " 'focussed': 16604,\n",
       " 'abandons': 6420,\n",
       " 'entire': 958,\n",
       " 'universality': 10243,\n",
       " 'gadget': 25285,\n",
       " 'idols': 10570,\n",
       " 'document': 3460,\n",
       " 'everytime': 11895,\n",
       " 'aestheticized': 22406,\n",
       " 'denny': 11661,\n",
       " 'courses': 13316,\n",
       " 'shocking': 1512,\n",
       " 'reactions': 4899,\n",
       " 'gfc': 22870,\n",
       " 'operation': 25523,\n",
       " 'minibiographies': 16239,\n",
       " 'prickling': 21336,\n",
       " 'kensington': 18668,\n",
       " 'research': 9944,\n",
       " 'movieish': 14197,\n",
       " 'occurs': 6724,\n",
       " 'poignantly': 6661,\n",
       " 'heads': 1976,\n",
       " 'pyne': 15276,\n",
       " 'definition': 3090,\n",
       " 'pairs': 11746,\n",
       " 'testament': 2179,\n",
       " 'existential': 3301,\n",
       " 'terrifyingly': 20849,\n",
       " '2012s': 9723,\n",
       " 'brutally': 4910,\n",
       " 'paganchristian': 19440,\n",
       " 'nome': 16127,\n",
       " 'arlington': 6147,\n",
       " 'cradles': 19103,\n",
       " 'moderately': 3405,\n",
       " 'bigscreen': 10271,\n",
       " 'hallowed': 25512,\n",
       " 'hamptons': 16222,\n",
       " 'saint': 11381,\n",
       " 'grossness': 21005,\n",
       " 'essays': 9973,\n",
       " 'justly': 12035,\n",
       " 'semisweet': 16155,\n",
       " 'resler': 11557,\n",
       " 'bruckheimer': 6726,\n",
       " 'getaway': 12672,\n",
       " 'spectatorship': 15913,\n",
       " 'underpinning': 14598,\n",
       " 'tobut': 24008,\n",
       " 'nikolaj': 9036,\n",
       " 'waft': 15493,\n",
       " 'daredevils': 25137,\n",
       " 'graffiti': 11887,\n",
       " 'blond': 24911,\n",
       " 'cleverness': 3542,\n",
       " 'sell': 3865,\n",
       " 'fermented': 21765,\n",
       " 'tarnish': 11242,\n",
       " 'self': 240,\n",
       " 'heterosexual': 12299,\n",
       " 'also': 116,\n",
       " 'recognizing': 17432,\n",
       " 'mends': 20500,\n",
       " 'raucous': 4424,\n",
       " 'virus': 10562,\n",
       " 'meriwethers': 24692,\n",
       " 'understands': 3134,\n",
       " 'arson': 24216,\n",
       " 'seize': 23228,\n",
       " 'sometimes': 495,\n",
       " 'stoning': 22931,\n",
       " 'discomforts': 21515,\n",
       " 'flits': 10184,\n",
       " 'barred': 9904,\n",
       " 'bullseye': 18434,\n",
       " 'barren': 15037,\n",
       " 'barrel': 9590,\n",
       " 'ambiguities': 11262,\n",
       " 'crowbar': 20227,\n",
       " 'blended': 6383,\n",
       " 'naomi': 4726,\n",
       " 'overwhelmed': 7088,\n",
       " 'occupation': 5147,\n",
       " 'careered': 25367,\n",
       " 'wraps': 4488,\n",
       " 'kinkiness': 19280,\n",
       " 'neophytes': 18511,\n",
       " 'gooey': 6534,\n",
       " 'cassette': 20346,\n",
       " 'antiadult': 23742,\n",
       " 'indifference': 9229,\n",
       " 'columns': 18020,\n",
       " 'atheistic': 21225,\n",
       " 'secular': 7385,\n",
       " 'sunny': 4267,\n",
       " 'remedy': 19690,\n",
       " 'compass': 6775,\n",
       " 'distraction': 10395,\n",
       " 'topher': 6880,\n",
       " 'adapts': 5574,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedtokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingReviews = np.zeros((ReviewsVocabSize, 100))\n",
    "embedding_dim=100\n",
    "for word, i in extractedtokenizer.word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddingReviews[i] = embedding_vector\n",
    "    else:\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings[word]=new_embedding\n",
    "        embeddingReviews[i]=new_embedding       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25589"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddingReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedReviews=extractedtokenizer.texts_to_sequences(ExtractedReviews)\n",
    "encodedSummaries=summarytokenizer.texts_to_sequences(Summaries)\n",
    "#encodedTargetSummaries=ts.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_modified_summaries=[]\n",
    "count=0\n",
    "for line in Summaries:\n",
    "    wordList=[]\n",
    "    count+=1\n",
    "    #print('count:->',count)\n",
    "    for word in nltk.word_tokenize(line):\n",
    "        #print(word)\n",
    "        if modiefiedSummaryWord_index.has_key(word) !=False:\n",
    "            wordList.append(modiefiedSummaryWord_index[word])\n",
    "    encoded_modified_summaries.append(wordList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4867, 3, 60, 1852, 223, 4, 2, 1853, 6, 12, 1854, 1056, 2, 373, 4, 1855, 38, 18, 1856, 3, 449, 739, 9, 1857, 11, 1858, 4868]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_modified_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 60, 1852, 223, 4, 2, 1853, 6, 12, 1854, 1056, 2, 373, 4, 1855, 38, 18, 1856, 3, 449, 739, 9, 1857, 11, 1858]\n"
     ]
    }
   ],
   "source": [
    "print(encodedSummaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxReviewLength=200\n",
    "maxSummaryLength=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedReviews = pad_sequences(encodedReviews, maxlen=maxReviewLength, padding='post',truncating='post')\n",
    "paddedSummary = pad_sequences(encodedSummaries, maxlen=maxSummaryLength, padding='post',truncating='post')\n",
    "paddedModifiedSummary = pad_sequences(encoded_modified_summaries, maxlen=maxSummaryLength, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4531   706    28     1   694     6  3726   866     3 10431   628     9\n",
      "  1843  1981  8580     6  9068    43    22    70   529     1    12     5\n",
      "     2  4651     4    84    56    24   128   335     3     8  1084     2\n",
      "   773    31   152    22   444    10  4545  1188   130     3   138   675\n",
      "    29  1820     7  1438 18523     7    16   556  1174    12    44    22\n",
      "   308    17   255   114   111    22   145  5224     9     1   138   675\n",
      "  4531   706    90     1    92     4     2  6166  4329    21  1357   219\n",
      "  2055   254     3  1431  2031  4531   706     5     1  2383     4    19\n",
      "  3303  4921   587  1437    13   351  1222   504  4531   706    39    17\n",
      "    23    10  7161    10     1  9068     8  2559    10 10640    61    20\n",
      "     2 18524   335   201     7     2 10797  9068    16     5     2   412\n",
      "    73   326   659    21  2018 18525 13532    67   668    12     9  5350\n",
      "  1099    54     4     1  3222     2   178    12    13     8   877     6\n",
      "   529     2    34    26     2   545  4702    17   126  1101  1218     3\n",
      "  3672   379    13   126     2   102   705    87    11   678   545  7706\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(paddedReviews[290])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3   60 1852  223    4    2 1853    6   12 1854 1056    2  373    4 1855\n",
      "   38   18 1856    3  449  739    9 1857   11 1858    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(paddedSummary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4867    3   60 1852  223    4    2 1853    6   12 1854 1056    2  373    4\n",
      " 1855   38   18 1856    3  449  739    9 1857   11 1858 4868    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(paddedModifiedSummary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((840, 200), (840, 30), (840, 30))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataIX=paddedReviews\n",
    "dataTY=paddedSummary\n",
    "dataIY=paddedModifiedSummary\n",
    "dataIX.shape,dataIY.shape,dataTY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples=len(dataIX)\n",
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderInputSummary=to_categorical(paddedModifiedSummary,num_classes=ModifiedVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderInputSummary=decoderInputSummary.reshape(nb_samples,-1,ModifiedVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 30, 4870)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderInputSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderTargetSummary=to_categorical(paddedSummary,num_classes=ModifiedVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25200, 4870)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderTargetSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIX=dataIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderTargetSummary=decoderTargetSummary.reshape(nb_samples,-1,ModifiedVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 30, 4870)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderTargetSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input shape is:-> (?, 200)\n"
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "Encoder_embedding_layer = Embedding(ReviewsVocabSize,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddingReviews],\n",
    "                            input_length=maxReviewLength,\n",
    "                            trainable=True)\n",
    "encoder_input=Input(shape=(maxReviewLength,))\n",
    "print('encoder_input shape is:->',encoder_input.shape)\n",
    "embedded_Encoder_inputSequence=Encoder_embedding_layer(encoder_input)\n",
    "encoder_LSTM=LSTM(256,return_state=True)\n",
    "encoder_output,encoder_h,encoder_c=encoder_LSTM(embedded_Encoder_inputSequence)\n",
    "encoder_states=[encoder_h,encoder_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input=Input(shape=(None,ModifiedVocabSize))\n",
    "#embedded_Decoder_inputSequence=Decoder_embedding_layer(decoder_input)\n",
    "decoder_LSTM=LSTM(256,return_sequences=True, return_state = True,dropout=0.2)\n",
    "decoder_output,decoder_h,decoder_c=decoder_LSTM(decoder_input,initial_state=encoder_states)\n",
    "decoder_dense=Dense(ModifiedVocabSize,activation='softmax')\n",
    "final_decoder_out=decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model=Model(inputs=[encoder_input,decoder_input],output=final_decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999,decay=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "#filepath=\"summWithoutAttention.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_weights_only=False, mode='auto', period=1)\n",
    "checkpointer = ModelCheckpoint(filepath='SummarizationWithoutAttentionV3.1Weights.hdf5', verbose=1, save_best_only=False,mode='auto',period=1)\n",
    "\n",
    "#callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 200, 100)      2558900     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, None, 4870)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 256), (None,  365568      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    [(None, None, 256), ( 5250048     input_3[0][0]                    \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, None, 4870)    1251590     lstm_3[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 9,426,106\n",
      "Trainable params: 9,426,106\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 281.00 337.00\" width=\"281pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 277,-333 277,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140630737227344 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140630737227344</title>\n",
       "<polygon fill=\"none\" points=\"18,-292.5 18,-328.5 143,-328.5 143,-292.5 18,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-306.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140630783039120 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140630783039120</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 161,-255.5 161,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-233.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 140630737227344&#45;&gt;140630783039120 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140630737227344-&gt;140630783039120</title>\n",
       "<path d=\"M80.5,-292.313C80.5,-284.289 80.5,-274.547 80.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"84.0001,-265.529 80.5,-255.529 77.0001,-265.529 84.0001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140630814533776 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140630814533776</title>\n",
       "<polygon fill=\"none\" points=\"31.5,-146.5 31.5,-182.5 129.5,-182.5 129.5,-146.5 31.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-160.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140630783039120&#45;&gt;140630814533776 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140630783039120-&gt;140630814533776</title>\n",
       "<path d=\"M80.5,-219.313C80.5,-211.289 80.5,-201.547 80.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"84.0001,-192.529 80.5,-182.529 77.0001,-192.529 84.0001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140630783039632 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140630783039632</title>\n",
       "<polygon fill=\"none\" points=\"148,-146.5 148,-182.5 273,-182.5 273,-146.5 148,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210.5\" y=\"-160.8\">input_3: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140628705539536 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140628705539536</title>\n",
       "<polygon fill=\"none\" points=\"96.5,-73.5 96.5,-109.5 194.5,-109.5 194.5,-73.5 96.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.5\" y=\"-87.8\">lstm_3: LSTM</text>\n",
       "</g>\n",
       "<!-- 140630783039632&#45;&gt;140628705539536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140630783039632-&gt;140628705539536</title>\n",
       "<path d=\"M194.765,-146.313C186.701,-137.505 176.741,-126.625 167.892,-116.958\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"170.424,-114.541 161.09,-109.529 165.261,-119.268 170.424,-114.541\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140630814533776&#45;&gt;140628705539536 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140630814533776-&gt;140628705539536</title>\n",
       "<path d=\"M96.2347,-146.313C104.299,-137.505 114.259,-126.625 123.108,-116.958\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"125.739,-119.268 129.91,-109.529 120.576,-114.541 125.739,-119.268\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140628705203088 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140628705203088</title>\n",
       "<polygon fill=\"none\" points=\"94.5,-0.5 94.5,-36.5 196.5,-36.5 196.5,-0.5 94.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140628705539536&#45;&gt;140628705203088 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140628705539536-&gt;140628705203088</title>\n",
       "<path d=\"M145.5,-73.3129C145.5,-65.2895 145.5,-55.5475 145.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149,-46.5288 145.5,-36.5288 142,-46.5289 149,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 672 samples, validate on 168 samples\n",
      "Epoch 1/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 5.5991 - acc: 0.3121Epoch 00000: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 5.6008 - acc: 0.3118 - val_loss: 5.2974 - val_acc: 0.3240\n",
      "Epoch 2/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.9035 - acc: 0.3207Epoch 00001: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 4.8986 - acc: 0.3213 - val_loss: 5.3094 - val_acc: 0.3250\n",
      "Epoch 3/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.7414 - acc: 0.3287Epoch 00002: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 4.7452 - acc: 0.3283 - val_loss: 5.3537 - val_acc: 0.3317\n",
      "Epoch 4/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.6714 - acc: 0.3301Epoch 00003: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 4.6627 - acc: 0.3313 - val_loss: 5.3922 - val_acc: 0.3337\n",
      "Epoch 5/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.5652 - acc: 0.3369Epoch 00004: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 4.5659 - acc: 0.3367 - val_loss: 5.4355 - val_acc: 0.3403\n",
      "Epoch 6/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.4806 - acc: 0.3427Epoch 00005: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 4.4781 - acc: 0.3430 - val_loss: 5.4898 - val_acc: 0.3411\n",
      "Epoch 7/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.4071 - acc: 0.3475Epoch 00006: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 209s - loss: 4.4072 - acc: 0.3474 - val_loss: 5.4811 - val_acc: 0.3442\n",
      "Epoch 8/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.3282 - acc: 0.3528Epoch 00007: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 217s - loss: 4.3294 - acc: 0.3528 - val_loss: 5.5326 - val_acc: 0.3417\n",
      "Epoch 9/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.2698 - acc: 0.3552Epoch 00008: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 4.2697 - acc: 0.3552 - val_loss: 5.5581 - val_acc: 0.3431\n",
      "Epoch 10/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.2117 - acc: 0.3601Epoch 00009: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 4.2122 - acc: 0.3600 - val_loss: 5.5646 - val_acc: 0.3464\n",
      "Epoch 11/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.1625 - acc: 0.3626Epoch 00010: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 4.1586 - acc: 0.3633 - val_loss: 5.5856 - val_acc: 0.3468\n",
      "Epoch 12/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.1063 - acc: 0.3664Epoch 00011: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 4.1029 - acc: 0.3669 - val_loss: 5.6413 - val_acc: 0.3450\n",
      "Epoch 13/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.0480 - acc: 0.3705Epoch 00012: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 4.0508 - acc: 0.3701 - val_loss: 5.6426 - val_acc: 0.3472\n",
      "Epoch 14/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 4.0033 - acc: 0.3731Epoch 00013: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 4.0058 - acc: 0.3727 - val_loss: 5.7068 - val_acc: 0.3464\n",
      "Epoch 15/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.9625 - acc: 0.3734Epoch 00014: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 3.9627 - acc: 0.3733 - val_loss: 5.6995 - val_acc: 0.3472\n",
      "Epoch 16/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.9183 - acc: 0.3773Epoch 00015: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 3.9173 - acc: 0.3775 - val_loss: 5.7163 - val_acc: 0.3458\n",
      "Epoch 17/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.8742 - acc: 0.3785Epoch 00016: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 3.8726 - acc: 0.3787 - val_loss: 5.7549 - val_acc: 0.3454\n",
      "Epoch 18/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.8326 - acc: 0.3803Epoch 00017: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 3.8318 - acc: 0.3805 - val_loss: 5.7792 - val_acc: 0.3446\n",
      "Epoch 19/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.7926 - acc: 0.3848Epoch 00018: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 3.7932 - acc: 0.3846 - val_loss: 5.8119 - val_acc: 0.3462\n",
      "Epoch 20/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.7485 - acc: 0.3860Epoch 00019: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 204s - loss: 3.7523 - acc: 0.3852 - val_loss: 5.8278 - val_acc: 0.3440\n",
      "Epoch 21/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.7147 - acc: 0.3862Epoch 00020: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 3.7136 - acc: 0.3864 - val_loss: 5.8907 - val_acc: 0.3448\n",
      "Epoch 22/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.6781 - acc: 0.3889Epoch 00021: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 3.6800 - acc: 0.3885 - val_loss: 5.9117 - val_acc: 0.3440\n",
      "Epoch 23/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.6463 - acc: 0.3888Epoch 00022: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 3.6440 - acc: 0.3891 - val_loss: 5.9065 - val_acc: 0.3431\n",
      "Epoch 24/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.6077 - acc: 0.3927Epoch 00023: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.6114 - acc: 0.3919 - val_loss: 5.9466 - val_acc: 0.3442\n",
      "Epoch 25/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.5714 - acc: 0.3951Epoch 00024: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 3.5735 - acc: 0.3947 - val_loss: 5.9374 - val_acc: 0.3407\n",
      "Epoch 26/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.5473 - acc: 0.3956Epoch 00025: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 3.5447 - acc: 0.3960 - val_loss: 5.9760 - val_acc: 0.3438\n",
      "Epoch 27/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.5149 - acc: 0.3971Epoch 00026: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.5161 - acc: 0.3969 - val_loss: 5.9673 - val_acc: 0.3442\n",
      "Epoch 28/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.4792 - acc: 0.4003Epoch 00027: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.4812 - acc: 0.4000 - val_loss: 5.9993 - val_acc: 0.3433\n",
      "Epoch 29/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.4611 - acc: 0.3989Epoch 00028: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 3.4558 - acc: 0.3998 - val_loss: 6.0240 - val_acc: 0.3419\n",
      "Epoch 30/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 3.4163 - acc: 0.4035Epoch 00029: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.4175 - acc: 0.4034 - val_loss: 6.0070 - val_acc: 0.3423\n",
      "Epoch 31/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.4010 - acc: 0.4026Epoch 00030: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 3.4019 - acc: 0.4024 - val_loss: 6.0524 - val_acc: 0.3448\n",
      "Epoch 32/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.3739 - acc: 0.4057Epoch 00031: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 3.3715 - acc: 0.4062 - val_loss: 6.0422 - val_acc: 0.3419\n",
      "Epoch 33/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.3405 - acc: 0.4080Epoch 00032: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.3408 - acc: 0.4079 - val_loss: 6.0834 - val_acc: 0.3429\n",
      "Epoch 34/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.3184 - acc: 0.4094Epoch 00033: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.3177 - acc: 0.4095 - val_loss: 6.0821 - val_acc: 0.3431\n",
      "Epoch 35/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.2909 - acc: 0.4118Epoch 00034: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.2891 - acc: 0.4121 - val_loss: 6.0670 - val_acc: 0.3387\n",
      "Epoch 36/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.2625 - acc: 0.4141Epoch 00035: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.2644 - acc: 0.4138 - val_loss: 6.1034 - val_acc: 0.3415\n",
      "Epoch 37/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.2362 - acc: 0.4141Epoch 00036: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.2368 - acc: 0.4140 - val_loss: 6.1099 - val_acc: 0.3411\n",
      "Epoch 38/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.2204 - acc: 0.4172Epoch 00037: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.2182 - acc: 0.4175 - val_loss: 6.1219 - val_acc: 0.3407\n",
      "Epoch 39/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.1939 - acc: 0.4183Epoch 00038: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.1963 - acc: 0.4178 - val_loss: 6.1645 - val_acc: 0.3393\n",
      "Epoch 40/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.1718 - acc: 0.4206Epoch 00039: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.1704 - acc: 0.4208 - val_loss: 6.1231 - val_acc: 0.3383\n",
      "Epoch 41/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.1471 - acc: 0.4210Epoch 00040: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.1451 - acc: 0.4213 - val_loss: 6.1528 - val_acc: 0.3375\n",
      "Epoch 42/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.1301 - acc: 0.4242Epoch 00041: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.1273 - acc: 0.4247 - val_loss: 6.2033 - val_acc: 0.3387\n",
      "Epoch 43/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.1084 - acc: 0.4268Epoch 00042: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 3.1058 - acc: 0.4274 - val_loss: 6.1835 - val_acc: 0.3397\n",
      "Epoch 44/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.0860 - acc: 0.4271Epoch 00043: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.0804 - acc: 0.4282 - val_loss: 6.2136 - val_acc: 0.3399\n",
      "Epoch 45/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.0615 - acc: 0.4306Epoch 00044: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.0611 - acc: 0.4307 - val_loss: 6.2050 - val_acc: 0.3399\n",
      "Epoch 46/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.0456 - acc: 0.4331Epoch 00045: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.0426 - acc: 0.4335 - val_loss: 6.2264 - val_acc: 0.3363\n",
      "Epoch 47/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 3.0260 - acc: 0.4349Epoch 00046: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.0257 - acc: 0.4348 - val_loss: 6.2323 - val_acc: 0.3389\n",
      "Epoch 48/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9980 - acc: 0.4388Epoch 00047: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 3.0013 - acc: 0.4381 - val_loss: 6.2283 - val_acc: 0.3371\n",
      "Epoch 49/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9813 - acc: 0.4395Epoch 00048: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.9821 - acc: 0.4393 - val_loss: 6.2520 - val_acc: 0.3397\n",
      "Epoch 50/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9663 - acc: 0.4400Epoch 00049: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.9674 - acc: 0.4398 - val_loss: 6.2395 - val_acc: 0.3357\n",
      "Epoch 51/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9505 - acc: 0.4438Epoch 00050: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.9490 - acc: 0.4442 - val_loss: 6.2822 - val_acc: 0.3367\n",
      "Epoch 52/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9316 - acc: 0.4463Epoch 00051: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.9324 - acc: 0.4461 - val_loss: 6.2399 - val_acc: 0.3361\n",
      "Epoch 53/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9138 - acc: 0.4497Epoch 00052: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.9121 - acc: 0.4500 - val_loss: 6.2818 - val_acc: 0.3361\n",
      "Epoch 54/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.9041 - acc: 0.4497Epoch 00053: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.9020 - acc: 0.4502 - val_loss: 6.2853 - val_acc: 0.3339\n",
      "Epoch 55/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.8817 - acc: 0.4546Epoch 00054: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.8812 - acc: 0.4549 - val_loss: 6.2860 - val_acc: 0.3351\n",
      "Epoch 56/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.8620 - acc: 0.4581Epoch 00055: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.8619 - acc: 0.4580 - val_loss: 6.2972 - val_acc: 0.3355\n",
      "Epoch 57/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.8534 - acc: 0.4589Epoch 00056: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.8493 - acc: 0.4597 - val_loss: 6.2955 - val_acc: 0.3337\n",
      "Epoch 58/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.8354 - acc: 0.4608Epoch 00057: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.8342 - acc: 0.4612 - val_loss: 6.3057 - val_acc: 0.3341\n",
      "Epoch 59/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.8205 - acc: 0.4643Epoch 00058: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.8173 - acc: 0.4649 - val_loss: 6.3088 - val_acc: 0.3319\n",
      "Epoch 60/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7973 - acc: 0.4683Epoch 00059: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7982 - acc: 0.4681 - val_loss: 6.3293 - val_acc: 0.3333\n",
      "Epoch 61/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7869 - acc: 0.4693Epoch 00060: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.7863 - acc: 0.4695 - val_loss: 6.3383 - val_acc: 0.3321\n",
      "Epoch 62/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7736 - acc: 0.4725Epoch 00061: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7717 - acc: 0.4727 - val_loss: 6.3460 - val_acc: 0.3339\n",
      "Epoch 63/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7524 - acc: 0.4779Epoch 00062: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7550 - acc: 0.4774 - val_loss: 6.3485 - val_acc: 0.3337\n",
      "Epoch 64/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7415 - acc: 0.4805Epoch 00063: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7389 - acc: 0.4812 - val_loss: 6.3449 - val_acc: 0.3319\n",
      "Epoch 65/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7232 - acc: 0.4817Epoch 00064: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7247 - acc: 0.4812 - val_loss: 6.3695 - val_acc: 0.3319\n",
      "Epoch 66/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7131 - acc: 0.4837Epoch 00065: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.7123 - acc: 0.4838 - val_loss: 6.3914 - val_acc: 0.3327\n",
      "Epoch 67/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.7021 - acc: 0.4858Epoch 00066: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.6983 - acc: 0.4865 - val_loss: 6.3644 - val_acc: 0.3321\n",
      "Epoch 68/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6835 - acc: 0.4909Epoch 00067: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.6842 - acc: 0.4908 - val_loss: 6.3664 - val_acc: 0.3302\n",
      "Epoch 69/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6758 - acc: 0.4917Epoch 00068: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.6735 - acc: 0.4921 - val_loss: 6.3966 - val_acc: 0.3304\n",
      "Epoch 70/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6545 - acc: 0.4938Epoch 00069: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.6534 - acc: 0.4941 - val_loss: 6.4121 - val_acc: 0.3317\n",
      "Epoch 71/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6435 - acc: 0.4976Epoch 00070: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.6446 - acc: 0.4975 - val_loss: 6.4045 - val_acc: 0.3310\n",
      "Epoch 72/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6335 - acc: 0.4992Epoch 00071: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.6336 - acc: 0.4992 - val_loss: 6.4307 - val_acc: 0.3304\n",
      "Epoch 73/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6213 - acc: 0.4989Epoch 00072: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.6199 - acc: 0.4994 - val_loss: 6.4266 - val_acc: 0.3304\n",
      "Epoch 74/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.6032 - acc: 0.5061Epoch 00073: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.6027 - acc: 0.5062 - val_loss: 6.4192 - val_acc: 0.3300\n",
      "Epoch 75/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5941 - acc: 0.5098Epoch 00074: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5935 - acc: 0.5098 - val_loss: 6.4164 - val_acc: 0.3292\n",
      "Epoch 76/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5847 - acc: 0.5109Epoch 00075: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5828 - acc: 0.5113 - val_loss: 6.4407 - val_acc: 0.3296\n",
      "Epoch 77/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5676 - acc: 0.5133Epoch 00076: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.5685 - acc: 0.5130 - val_loss: 6.4226 - val_acc: 0.3272\n",
      "Epoch 78/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5535 - acc: 0.5154Epoch 00077: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.5527 - acc: 0.5157 - val_loss: 6.4525 - val_acc: 0.3292\n",
      "Epoch 79/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5516 - acc: 0.5160Epoch 00078: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5500 - acc: 0.5162 - val_loss: 6.4430 - val_acc: 0.3274\n",
      "Epoch 80/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5300 - acc: 0.5217Epoch 00079: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5306 - acc: 0.5217 - val_loss: 6.4661 - val_acc: 0.3274\n",
      "Epoch 81/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5245 - acc: 0.5237Epoch 00080: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5226 - acc: 0.5242 - val_loss: 6.4778 - val_acc: 0.3286\n",
      "Epoch 82/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5127 - acc: 0.5266Epoch 00081: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5133 - acc: 0.5265 - val_loss: 6.4562 - val_acc: 0.3260\n",
      "Epoch 83/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.5057 - acc: 0.5244Epoch 00082: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.5057 - acc: 0.5245 - val_loss: 6.4763 - val_acc: 0.3266\n",
      "Epoch 84/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4933 - acc: 0.5277Epoch 00083: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.4919 - acc: 0.5280 - val_loss: 6.4814 - val_acc: 0.3274\n",
      "Epoch 85/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4751 - acc: 0.5328Epoch 00084: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.4757 - acc: 0.5328 - val_loss: 6.4944 - val_acc: 0.3254\n",
      "Epoch 86/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4595 - acc: 0.5359Epoch 00085: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.4611 - acc: 0.5356 - val_loss: 6.5027 - val_acc: 0.3284\n",
      "Epoch 87/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4544 - acc: 0.5376Epoch 00086: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.4539 - acc: 0.5378 - val_loss: 6.5060 - val_acc: 0.3266\n",
      "Epoch 88/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 2.4476 - acc: 0.5398Epoch 00087: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.4480 - acc: 0.5397 - val_loss: 6.5342 - val_acc: 0.3260\n",
      "Epoch 89/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4353 - acc: 0.5418Epoch 00088: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.4349 - acc: 0.5419 - val_loss: 6.5191 - val_acc: 0.3262\n",
      "Epoch 90/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4287 - acc: 0.5419Epoch 00089: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.4265 - acc: 0.5425 - val_loss: 6.5176 - val_acc: 0.3252\n",
      "Epoch 91/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4165 - acc: 0.5458Epoch 00090: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.4146 - acc: 0.5461 - val_loss: 6.5324 - val_acc: 0.3258\n",
      "Epoch 92/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.4049 - acc: 0.5475Epoch 00091: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.4065 - acc: 0.5471 - val_loss: 6.5310 - val_acc: 0.3256\n",
      "Epoch 93/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3969 - acc: 0.5493Epoch 00092: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.3987 - acc: 0.5489 - val_loss: 6.5405 - val_acc: 0.3248\n",
      "Epoch 94/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3807 - acc: 0.5523Epoch 00093: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.3817 - acc: 0.5521 - val_loss: 6.5402 - val_acc: 0.3244\n",
      "Epoch 95/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3771 - acc: 0.5539Epoch 00094: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.3779 - acc: 0.5536 - val_loss: 6.5437 - val_acc: 0.3238\n",
      "Epoch 96/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3648 - acc: 0.5579Epoch 00095: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.3670 - acc: 0.5574 - val_loss: 6.5640 - val_acc: 0.3236\n",
      "Epoch 97/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3507 - acc: 0.5600Epoch 00096: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.3520 - acc: 0.5598 - val_loss: 6.5674 - val_acc: 0.3250\n",
      "Epoch 98/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3523 - acc: 0.5600Epoch 00097: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.3495 - acc: 0.5605 - val_loss: 6.5590 - val_acc: 0.3230\n",
      "Epoch 99/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3411 - acc: 0.5614Epoch 00098: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.3395 - acc: 0.5619 - val_loss: 6.5662 - val_acc: 0.3248\n",
      "Epoch 100/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3291 - acc: 0.5646Epoch 00099: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.3301 - acc: 0.5643 - val_loss: 6.5772 - val_acc: 0.3228\n",
      "Epoch 101/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3241 - acc: 0.5653Epoch 00100: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.3237 - acc: 0.5654 - val_loss: 6.5898 - val_acc: 0.3250\n",
      "Epoch 102/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3140 - acc: 0.5673Epoch 00101: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.3152 - acc: 0.5669 - val_loss: 6.6035 - val_acc: 0.3256\n",
      "Epoch 103/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.3026 - acc: 0.5700Epoch 00102: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.3027 - acc: 0.5698 - val_loss: 6.6037 - val_acc: 0.3244\n",
      "Epoch 104/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2987 - acc: 0.5727Epoch 00103: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.2996 - acc: 0.5725 - val_loss: 6.6089 - val_acc: 0.3240\n",
      "Epoch 105/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2874 - acc: 0.5713Epoch 00104: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2881 - acc: 0.5712 - val_loss: 6.5999 - val_acc: 0.3232\n",
      "Epoch 106/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2759 - acc: 0.5756Epoch 00105: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2775 - acc: 0.5752 - val_loss: 6.6061 - val_acc: 0.3236\n",
      "Epoch 107/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2734 - acc: 0.5771Epoch 00106: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2730 - acc: 0.5771 - val_loss: 6.6235 - val_acc: 0.3234\n",
      "Epoch 108/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2661 - acc: 0.5783Epoch 00107: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2647 - acc: 0.5786 - val_loss: 6.6112 - val_acc: 0.3230\n",
      "Epoch 109/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2554 - acc: 0.5804Epoch 00108: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2541 - acc: 0.5809 - val_loss: 6.6173 - val_acc: 0.3232\n",
      "Epoch 110/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2475 - acc: 0.5810Epoch 00109: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2458 - acc: 0.5814 - val_loss: 6.6184 - val_acc: 0.3228\n",
      "Epoch 111/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2369 - acc: 0.5829Epoch 00110: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.2372 - acc: 0.5828 - val_loss: 6.6275 - val_acc: 0.3222\n",
      "Epoch 112/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2296 - acc: 0.5857Epoch 00111: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.2288 - acc: 0.5860 - val_loss: 6.6357 - val_acc: 0.3228\n",
      "Epoch 113/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2226 - acc: 0.5863Epoch 00112: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.2238 - acc: 0.5861 - val_loss: 6.6495 - val_acc: 0.3210\n",
      "Epoch 114/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2184 - acc: 0.5869Epoch 00113: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.2178 - acc: 0.5871 - val_loss: 6.6378 - val_acc: 0.3234\n",
      "Epoch 115/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2105 - acc: 0.5903Epoch 00114: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.2088 - acc: 0.5907 - val_loss: 6.6259 - val_acc: 0.3190\n",
      "Epoch 116/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.2004 - acc: 0.5900Epoch 00115: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1992 - acc: 0.5902 - val_loss: 6.6461 - val_acc: 0.3206\n",
      "Epoch 117/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1981 - acc: 0.5909Epoch 00116: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1950 - acc: 0.5915 - val_loss: 6.6686 - val_acc: 0.3202\n",
      "Epoch 118/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1873 - acc: 0.5947Epoch 00117: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 188s - loss: 2.1872 - acc: 0.5947 - val_loss: 6.6578 - val_acc: 0.3216\n",
      "Epoch 119/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1755 - acc: 0.6000Epoch 00118: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1759 - acc: 0.6000 - val_loss: 6.6806 - val_acc: 0.3202\n",
      "Epoch 120/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1670 - acc: 0.5978Epoch 00119: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1668 - acc: 0.5980 - val_loss: 6.6729 - val_acc: 0.3198\n",
      "Epoch 121/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1599 - acc: 0.6000Epoch 00120: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 188s - loss: 2.1613 - acc: 0.5996 - val_loss: 6.6802 - val_acc: 0.3208\n",
      "Epoch 122/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1586 - acc: 0.6009Epoch 00121: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1577 - acc: 0.6012 - val_loss: 6.6838 - val_acc: 0.3216\n",
      "Epoch 123/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1476 - acc: 0.6019Epoch 00122: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1484 - acc: 0.6019 - val_loss: 6.6868 - val_acc: 0.3200\n",
      "Epoch 124/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1395 - acc: 0.6033Epoch 00123: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1388 - acc: 0.6034 - val_loss: 6.6936 - val_acc: 0.3200\n",
      "Epoch 125/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1286 - acc: 0.6068Epoch 00124: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1284 - acc: 0.6068 - val_loss: 6.6933 - val_acc: 0.3194\n",
      "Epoch 126/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1262 - acc: 0.6055Epoch 00125: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1277 - acc: 0.6051 - val_loss: 6.7035 - val_acc: 0.3208\n",
      "Epoch 127/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1228 - acc: 0.6077Epoch 00126: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1209 - acc: 0.6081 - val_loss: 6.6898 - val_acc: 0.3185\n",
      "Epoch 128/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1103 - acc: 0.6115Epoch 00127: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.1126 - acc: 0.6110 - val_loss: 6.7135 - val_acc: 0.3200\n",
      "Epoch 129/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1103 - acc: 0.6089Epoch 00128: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.1089 - acc: 0.6091 - val_loss: 6.7108 - val_acc: 0.3179\n",
      "Epoch 130/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.1035 - acc: 0.6114Epoch 00129: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.1029 - acc: 0.6116 - val_loss: 6.7185 - val_acc: 0.3190\n",
      "Epoch 131/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0952 - acc: 0.6135Epoch 00130: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0952 - acc: 0.6133 - val_loss: 6.7303 - val_acc: 0.3185\n",
      "Epoch 132/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0887 - acc: 0.6158Epoch 00131: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 2.0882 - acc: 0.6158 - val_loss: 6.6924 - val_acc: 0.3177\n",
      "Epoch 133/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0811 - acc: 0.6166Epoch 00132: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.0813 - acc: 0.6167 - val_loss: 6.7218 - val_acc: 0.3181\n",
      "Epoch 134/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0716 - acc: 0.6178Epoch 00133: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.0725 - acc: 0.6175 - val_loss: 6.7130 - val_acc: 0.3177\n",
      "Epoch 135/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0665 - acc: 0.6188Epoch 00134: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.0673 - acc: 0.6185 - val_loss: 6.7297 - val_acc: 0.3190\n",
      "Epoch 136/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0603 - acc: 0.6211Epoch 00135: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 2.0611 - acc: 0.6208 - val_loss: 6.7316 - val_acc: 0.3181\n",
      "Epoch 137/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0507 - acc: 0.6210Epoch 00136: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0520 - acc: 0.6208 - val_loss: 6.7284 - val_acc: 0.3183\n",
      "Epoch 138/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0499 - acc: 0.6243Epoch 00137: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0497 - acc: 0.6243 - val_loss: 6.7369 - val_acc: 0.3183\n",
      "Epoch 139/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0439 - acc: 0.6233Epoch 00138: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0436 - acc: 0.6234 - val_loss: 6.7408 - val_acc: 0.3192\n",
      "Epoch 140/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0387 - acc: 0.6231Epoch 00139: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0390 - acc: 0.6231 - val_loss: 6.7619 - val_acc: 0.3177\n",
      "Epoch 141/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0338 - acc: 0.6271Epoch 00140: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0345 - acc: 0.6270 - val_loss: 6.7527 - val_acc: 0.3175\n",
      "Epoch 142/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0239 - acc: 0.6261Epoch 00141: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0233 - acc: 0.6265 - val_loss: 6.7565 - val_acc: 0.3177\n",
      "Epoch 143/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0244 - acc: 0.6278Epoch 00142: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0242 - acc: 0.6279 - val_loss: 6.7653 - val_acc: 0.3192\n",
      "Epoch 144/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0127 - acc: 0.6295Epoch 00143: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 2.0111 - acc: 0.6298 - val_loss: 6.7610 - val_acc: 0.3188\n",
      "Epoch 145/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 2.0088 - acc: 0.6309Epoch 00144: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0079 - acc: 0.6309 - val_loss: 6.7686 - val_acc: 0.3173\n",
      "Epoch 146/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 2.0047 - acc: 0.6315Epoch 00145: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 2.0037 - acc: 0.6319 - val_loss: 6.7960 - val_acc: 0.3177\n",
      "Epoch 147/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9941 - acc: 0.6316Epoch 00146: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9947 - acc: 0.6315 - val_loss: 6.7531 - val_acc: 0.3163\n",
      "Epoch 148/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9893 - acc: 0.6341Epoch 00147: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9905 - acc: 0.6339 - val_loss: 6.7869 - val_acc: 0.3187\n",
      "Epoch 149/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9863 - acc: 0.6355Epoch 00148: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9861 - acc: 0.6356 - val_loss: 6.7920 - val_acc: 0.3177\n",
      "Epoch 150/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9770 - acc: 0.6375Epoch 00149: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9773 - acc: 0.6373 - val_loss: 6.7835 - val_acc: 0.3190\n",
      "Epoch 151/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9767 - acc: 0.6375Epoch 00150: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9775 - acc: 0.6373 - val_loss: 6.7731 - val_acc: 0.3165\n",
      "Epoch 152/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9697 - acc: 0.6367Epoch 00151: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.9693 - acc: 0.6369 - val_loss: 6.7960 - val_acc: 0.3161\n",
      "Epoch 153/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9659 - acc: 0.6363Epoch 00152: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9642 - acc: 0.6367 - val_loss: 6.7913 - val_acc: 0.3171\n",
      "Epoch 154/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9606 - acc: 0.6393Epoch 00153: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9600 - acc: 0.6393 - val_loss: 6.7914 - val_acc: 0.3181\n",
      "Epoch 155/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9482 - acc: 0.6408Epoch 00154: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9492 - acc: 0.6406 - val_loss: 6.7966 - val_acc: 0.3165\n",
      "Epoch 156/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9479 - acc: 0.6437Epoch 00155: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9467 - acc: 0.6439 - val_loss: 6.8122 - val_acc: 0.3165\n",
      "Epoch 157/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9394 - acc: 0.6449Epoch 00156: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9402 - acc: 0.6449 - val_loss: 6.8037 - val_acc: 0.3165\n",
      "Epoch 158/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9315 - acc: 0.6455Epoch 00157: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.9312 - acc: 0.6455 - val_loss: 6.8071 - val_acc: 0.3171\n",
      "Epoch 159/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9330 - acc: 0.6463Epoch 00158: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9322 - acc: 0.6466 - val_loss: 6.8246 - val_acc: 0.3183\n",
      "Epoch 160/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9284 - acc: 0.6470Epoch 00159: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9288 - acc: 0.6469 - val_loss: 6.8007 - val_acc: 0.3171\n",
      "Epoch 161/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9181 - acc: 0.6492Epoch 00160: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9189 - acc: 0.6491 - val_loss: 6.8270 - val_acc: 0.3177\n",
      "Epoch 162/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9200 - acc: 0.6454Epoch 00161: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9193 - acc: 0.6456 - val_loss: 6.8208 - val_acc: 0.3161\n",
      "Epoch 163/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9168 - acc: 0.6499Epoch 00162: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.9157 - acc: 0.6500 - val_loss: 6.8320 - val_acc: 0.3177\n",
      "Epoch 164/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9075 - acc: 0.6477Epoch 00163: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9079 - acc: 0.6476 - val_loss: 6.8230 - val_acc: 0.3161\n",
      "Epoch 165/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.9023 - acc: 0.6521Epoch 00164: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.9015 - acc: 0.6524 - val_loss: 6.8282 - val_acc: 0.3171\n",
      "Epoch 166/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8970 - acc: 0.6524Epoch 00165: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.8982 - acc: 0.6521 - val_loss: 6.8233 - val_acc: 0.3169\n",
      "Epoch 167/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8946 - acc: 0.6514Epoch 00166: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.8950 - acc: 0.6511 - val_loss: 6.8214 - val_acc: 0.3141\n",
      "Epoch 168/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8863 - acc: 0.6556Epoch 00167: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.8852 - acc: 0.6557 - val_loss: 6.8350 - val_acc: 0.3153\n",
      "Epoch 169/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8837 - acc: 0.6546Epoch 00168: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.8841 - acc: 0.6548 - val_loss: 6.8365 - val_acc: 0.3157\n",
      "Epoch 170/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8750 - acc: 0.6571Epoch 00169: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.8772 - acc: 0.6566 - val_loss: 6.8498 - val_acc: 0.3171\n",
      "Epoch 171/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8757 - acc: 0.6566Epoch 00170: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.8760 - acc: 0.6565 - val_loss: 6.8496 - val_acc: 0.3179\n",
      "Epoch 172/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8664 - acc: 0.6596Epoch 00171: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.8658 - acc: 0.6596 - val_loss: 6.8508 - val_acc: 0.3155\n",
      "Epoch 173/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8592 - acc: 0.6584Epoch 00172: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.8612 - acc: 0.6581 - val_loss: 6.8492 - val_acc: 0.3135\n",
      "Epoch 174/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8570 - acc: 0.6610Epoch 00173: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.8567 - acc: 0.6612 - val_loss: 6.8524 - val_acc: 0.3149\n",
      "Epoch 175/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8522 - acc: 0.6621Epoch 00174: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.8520 - acc: 0.6623 - val_loss: 6.8698 - val_acc: 0.3153\n",
      "Epoch 176/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8495 - acc: 0.6615Epoch 00175: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.8473 - acc: 0.6619 - val_loss: 6.8491 - val_acc: 0.3155\n",
      "Epoch 177/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8494 - acc: 0.6626Epoch 00176: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.8482 - acc: 0.6627 - val_loss: 6.8655 - val_acc: 0.3157\n",
      "Epoch 178/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8407 - acc: 0.6645Epoch 00177: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 188s - loss: 1.8405 - acc: 0.6645 - val_loss: 6.8583 - val_acc: 0.3141\n",
      "Epoch 179/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8373 - acc: 0.6631Epoch 00178: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.8373 - acc: 0.6631 - val_loss: 6.8706 - val_acc: 0.3159\n",
      "Epoch 180/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8339 - acc: 0.6638Epoch 00179: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 188s - loss: 1.8328 - acc: 0.6642 - val_loss: 6.8916 - val_acc: 0.3167\n",
      "Epoch 181/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8338 - acc: 0.6645Epoch 00180: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.8337 - acc: 0.6645 - val_loss: 6.8818 - val_acc: 0.3151\n",
      "Epoch 182/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8212 - acc: 0.6686Epoch 00181: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.8192 - acc: 0.6690 - val_loss: 6.8592 - val_acc: 0.3143\n",
      "Epoch 183/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8218 - acc: 0.6667Epoch 00182: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 207s - loss: 1.8211 - acc: 0.6668 - val_loss: 6.8837 - val_acc: 0.3151\n",
      "Epoch 184/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8141 - acc: 0.6659Epoch 00183: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.8146 - acc: 0.6659 - val_loss: 6.8823 - val_acc: 0.3145\n",
      "Epoch 185/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8098 - acc: 0.6700Epoch 00184: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 213s - loss: 1.8096 - acc: 0.6700 - val_loss: 6.8808 - val_acc: 0.3133\n",
      "Epoch 186/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8070 - acc: 0.6687Epoch 00185: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.8077 - acc: 0.6686 - val_loss: 6.8822 - val_acc: 0.3145\n",
      "Epoch 187/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.8020 - acc: 0.6694Epoch 00186: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.8022 - acc: 0.6691 - val_loss: 6.8858 - val_acc: 0.3149\n",
      "Epoch 188/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7989 - acc: 0.6689Epoch 00187: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7986 - acc: 0.6689 - val_loss: 6.8886 - val_acc: 0.3141\n",
      "Epoch 189/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7940 - acc: 0.6711Epoch 00188: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.7938 - acc: 0.6713 - val_loss: 6.8960 - val_acc: 0.3149\n",
      "Epoch 190/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7981 - acc: 0.6706Epoch 00189: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7970 - acc: 0.6709 - val_loss: 6.9096 - val_acc: 0.3145\n",
      "Epoch 191/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7818 - acc: 0.6736Epoch 00190: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.7825 - acc: 0.6734 - val_loss: 6.9032 - val_acc: 0.3153\n",
      "Epoch 192/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7798 - acc: 0.6745Epoch 00191: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7804 - acc: 0.6746 - val_loss: 6.9100 - val_acc: 0.3133\n",
      "Epoch 193/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7769 - acc: 0.6774Epoch 00192: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7774 - acc: 0.6773 - val_loss: 6.9065 - val_acc: 0.3133\n",
      "Epoch 194/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7775 - acc: 0.6754Epoch 00193: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7767 - acc: 0.6756 - val_loss: 6.9069 - val_acc: 0.3137\n",
      "Epoch 195/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7666 - acc: 0.6758Epoch 00194: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7682 - acc: 0.6754 - val_loss: 6.9067 - val_acc: 0.3145\n",
      "Epoch 196/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7669 - acc: 0.6772Epoch 00195: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7667 - acc: 0.6770 - val_loss: 6.9136 - val_acc: 0.3143\n",
      "Epoch 197/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7615 - acc: 0.6775Epoch 00196: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.7621 - acc: 0.6772 - val_loss: 6.9092 - val_acc: 0.3143\n",
      "Epoch 198/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7581 - acc: 0.6781Epoch 00197: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7570 - acc: 0.6783 - val_loss: 6.9128 - val_acc: 0.3139\n",
      "Epoch 199/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7509 - acc: 0.6782Epoch 00198: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7519 - acc: 0.6779 - val_loss: 6.9093 - val_acc: 0.3135\n",
      "Epoch 200/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7487 - acc: 0.6808Epoch 00199: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.7503 - acc: 0.6803 - val_loss: 6.9146 - val_acc: 0.3147\n",
      "Epoch 201/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7407 - acc: 0.6816Epoch 00200: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7428 - acc: 0.6810 - val_loss: 6.9206 - val_acc: 0.3139\n",
      "Epoch 202/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7461 - acc: 0.6815Epoch 00201: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.7462 - acc: 0.6814 - val_loss: 6.9269 - val_acc: 0.3133\n",
      "Epoch 203/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7349 - acc: 0.6831Epoch 00202: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.7352 - acc: 0.6830 - val_loss: 6.9249 - val_acc: 0.3139\n",
      "Epoch 204/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 1.7415 - acc: 0.6796Epoch 00203: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.7420 - acc: 0.6794 - val_loss: 6.9252 - val_acc: 0.3139\n",
      "Epoch 205/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7347 - acc: 0.6820Epoch 00204: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.7336 - acc: 0.6822 - val_loss: 6.9342 - val_acc: 0.3135\n",
      "Epoch 206/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7273 - acc: 0.6858Epoch 00205: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.7276 - acc: 0.6858 - val_loss: 6.9275 - val_acc: 0.3131\n",
      "Epoch 207/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7250 - acc: 0.6856Epoch 00206: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.7250 - acc: 0.6856 - val_loss: 6.9424 - val_acc: 0.3149\n",
      "Epoch 208/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7192 - acc: 0.6863Epoch 00207: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.7176 - acc: 0.6864 - val_loss: 6.9507 - val_acc: 0.3151\n",
      "Epoch 209/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7133 - acc: 0.6889Epoch 00208: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.7152 - acc: 0.6887 - val_loss: 6.9339 - val_acc: 0.3145\n",
      "Epoch 210/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7175 - acc: 0.6844Epoch 00209: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.7163 - acc: 0.6847 - val_loss: 6.9359 - val_acc: 0.3127\n",
      "Epoch 211/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7065 - acc: 0.6896Epoch 00210: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.7070 - acc: 0.6897 - val_loss: 6.9545 - val_acc: 0.3137\n",
      "Epoch 212/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7046 - acc: 0.6873Epoch 00211: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7056 - acc: 0.6871 - val_loss: 6.9388 - val_acc: 0.3117\n",
      "Epoch 213/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.7046 - acc: 0.6901Epoch 00212: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.7039 - acc: 0.6903 - val_loss: 6.9398 - val_acc: 0.3123\n",
      "Epoch 214/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6993 - acc: 0.6895Epoch 00213: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.6986 - acc: 0.6898 - val_loss: 6.9457 - val_acc: 0.3135\n",
      "Epoch 215/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6994 - acc: 0.6896Epoch 00214: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 191s - loss: 1.6996 - acc: 0.6895 - val_loss: 6.9572 - val_acc: 0.3129\n",
      "Epoch 216/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6944 - acc: 0.6906Epoch 00215: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6941 - acc: 0.6906 - val_loss: 6.9586 - val_acc: 0.3149\n",
      "Epoch 217/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6932 - acc: 0.6903Epoch 00216: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6933 - acc: 0.6905 - val_loss: 6.9620 - val_acc: 0.3135\n",
      "Epoch 218/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6894 - acc: 0.6906Epoch 00217: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6878 - acc: 0.6909 - val_loss: 6.9623 - val_acc: 0.3125\n",
      "Epoch 219/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6805 - acc: 0.6946Epoch 00218: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6807 - acc: 0.6946 - val_loss: 6.9656 - val_acc: 0.3127\n",
      "Epoch 220/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6764 - acc: 0.6971Epoch 00219: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6774 - acc: 0.6970 - val_loss: 6.9508 - val_acc: 0.3127\n",
      "Epoch 221/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6720 - acc: 0.6946Epoch 00220: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6723 - acc: 0.6946 - val_loss: 6.9581 - val_acc: 0.3131\n",
      "Epoch 222/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6752 - acc: 0.6945Epoch 00221: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6757 - acc: 0.6943 - val_loss: 6.9631 - val_acc: 0.3117\n",
      "Epoch 223/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6686 - acc: 0.6967Epoch 00222: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6704 - acc: 0.6965 - val_loss: 6.9582 - val_acc: 0.3113\n",
      "Epoch 224/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6671 - acc: 0.6950Epoch 00223: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6686 - acc: 0.6946 - val_loss: 6.9680 - val_acc: 0.3117\n",
      "Epoch 225/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6632 - acc: 0.6941Epoch 00224: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6615 - acc: 0.6945 - val_loss: 6.9687 - val_acc: 0.3113\n",
      "Epoch 226/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6635 - acc: 0.6958Epoch 00225: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6635 - acc: 0.6957 - val_loss: 6.9827 - val_acc: 0.3137\n",
      "Epoch 227/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6577 - acc: 0.6962Epoch 00226: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6589 - acc: 0.6958 - val_loss: 6.9871 - val_acc: 0.3131\n",
      "Epoch 228/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6523 - acc: 0.6967Epoch 00227: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6528 - acc: 0.6967 - val_loss: 6.9848 - val_acc: 0.3121\n",
      "Epoch 229/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6470 - acc: 0.7018Epoch 00228: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.6463 - acc: 0.7019 - val_loss: 6.9842 - val_acc: 0.3127\n",
      "Epoch 230/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6487 - acc: 0.6991Epoch 00229: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.6486 - acc: 0.6991 - val_loss: 6.9870 - val_acc: 0.3117\n",
      "Epoch 231/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6481 - acc: 0.6980Epoch 00230: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6456 - acc: 0.6984 - val_loss: 6.9922 - val_acc: 0.3119\n",
      "Epoch 232/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6434 - acc: 0.6981Epoch 00231: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6434 - acc: 0.6982 - val_loss: 6.9801 - val_acc: 0.3109\n",
      "Epoch 233/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6384 - acc: 0.7016Epoch 00232: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.6387 - acc: 0.7016 - val_loss: 6.9887 - val_acc: 0.3109\n",
      "Epoch 234/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6319 - acc: 0.7027Epoch 00233: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 190s - loss: 1.6329 - acc: 0.7024 - val_loss: 6.9834 - val_acc: 0.3107\n",
      "Epoch 235/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6322 - acc: 0.7015Epoch 00234: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.6339 - acc: 0.7010 - val_loss: 6.9931 - val_acc: 0.3115\n",
      "Epoch 236/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6323 - acc: 0.7053Epoch 00235: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.6323 - acc: 0.7053 - val_loss: 6.9823 - val_acc: 0.3111\n",
      "Epoch 237/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6261 - acc: 0.7042Epoch 00236: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 189s - loss: 1.6267 - acc: 0.7040 - val_loss: 7.0034 - val_acc: 0.3109\n",
      "Epoch 238/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6227 - acc: 0.7050Epoch 00237: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.6238 - acc: 0.7047 - val_loss: 7.0054 - val_acc: 0.3117\n",
      "Epoch 239/400\n",
      "670/672 [============================>.] - ETA: 5s - loss: 1.6225 - acc: 0.7038 Epoch 00238: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 1900s - loss: 1.6230 - acc: 0.7035 - val_loss: 6.9970 - val_acc: 0.3129\n",
      "Epoch 240/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6123 - acc: 0.7039Epoch 00239: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.6132 - acc: 0.7038 - val_loss: 7.0024 - val_acc: 0.3115\n",
      "Epoch 241/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6104 - acc: 0.7052Epoch 00240: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.6119 - acc: 0.7047 - val_loss: 7.0074 - val_acc: 0.3117\n",
      "Epoch 242/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6137 - acc: 0.7083Epoch 00241: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.6145 - acc: 0.7080 - val_loss: 7.0088 - val_acc: 0.3117\n",
      "Epoch 243/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6090 - acc: 0.7087Epoch 00242: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.6079 - acc: 0.7090 - val_loss: 7.0067 - val_acc: 0.3107\n",
      "Epoch 244/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6075 - acc: 0.7068Epoch 00243: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.6077 - acc: 0.7068 - val_loss: 7.0075 - val_acc: 0.3123\n",
      "Epoch 245/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6013 - acc: 0.7062Epoch 00244: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.6006 - acc: 0.7062 - val_loss: 7.0092 - val_acc: 0.3119\n",
      "Epoch 246/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.6017 - acc: 0.7090Epoch 00245: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 210s - loss: 1.6008 - acc: 0.7091 - val_loss: 7.0073 - val_acc: 0.3119\n",
      "Epoch 247/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5954 - acc: 0.7072Epoch 00246: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.5935 - acc: 0.7075 - val_loss: 7.0063 - val_acc: 0.3105\n",
      "Epoch 248/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5986 - acc: 0.7101Epoch 00247: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 211s - loss: 1.5983 - acc: 0.7101 - val_loss: 7.0217 - val_acc: 0.3109\n",
      "Epoch 249/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5921 - acc: 0.7106Epoch 00248: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 209s - loss: 1.5919 - acc: 0.7105 - val_loss: 7.0241 - val_acc: 0.3111\n",
      "Epoch 250/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5884 - acc: 0.7113Epoch 00249: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.5895 - acc: 0.7110 - val_loss: 7.0273 - val_acc: 0.3097\n",
      "Epoch 251/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5882 - acc: 0.7109Epoch 00250: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.5874 - acc: 0.7111 - val_loss: 7.0096 - val_acc: 0.3093\n",
      "Epoch 252/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5854 - acc: 0.7114Epoch 00251: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.5854 - acc: 0.7114 - val_loss: 7.0304 - val_acc: 0.3113\n",
      "Epoch 253/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5783 - acc: 0.7117Epoch 00252: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.5772 - acc: 0.7118 - val_loss: 7.0223 - val_acc: 0.3109\n",
      "Epoch 254/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5754 - acc: 0.7126Epoch 00253: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.5765 - acc: 0.7124 - val_loss: 7.0314 - val_acc: 0.3101\n",
      "Epoch 255/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5681 - acc: 0.7140Epoch 00254: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.5689 - acc: 0.7139 - val_loss: 7.0313 - val_acc: 0.3103\n",
      "Epoch 256/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5749 - acc: 0.7122Epoch 00255: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.5741 - acc: 0.7125 - val_loss: 7.0302 - val_acc: 0.3129\n",
      "Epoch 257/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5723 - acc: 0.7138Epoch 00256: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.5714 - acc: 0.7139 - val_loss: 7.0423 - val_acc: 0.3109\n",
      "Epoch 258/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5679 - acc: 0.7141Epoch 00257: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 204s - loss: 1.5674 - acc: 0.7143 - val_loss: 7.0321 - val_acc: 0.3097\n",
      "Epoch 259/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5627 - acc: 0.7137Epoch 00258: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.5624 - acc: 0.7138 - val_loss: 7.0446 - val_acc: 0.3113\n",
      "Epoch 260/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5645 - acc: 0.7132Epoch 00259: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.5629 - acc: 0.7135 - val_loss: 7.0444 - val_acc: 0.3107\n",
      "Epoch 261/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5554 - acc: 0.7149Epoch 00260: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.5556 - acc: 0.7150 - val_loss: 7.0465 - val_acc: 0.3113\n",
      "Epoch 262/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 1.5549 - acc: 0.7153Epoch 00261: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.5534 - acc: 0.7155 - val_loss: 7.0411 - val_acc: 0.3097\n",
      "Epoch 263/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5539 - acc: 0.7158Epoch 00262: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.5531 - acc: 0.7160 - val_loss: 7.0568 - val_acc: 0.3105\n",
      "Epoch 264/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5556 - acc: 0.7171Epoch 00263: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.5539 - acc: 0.7174 - val_loss: 7.0664 - val_acc: 0.3111\n",
      "Epoch 265/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5486 - acc: 0.7202Epoch 00264: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.5479 - acc: 0.7203 - val_loss: 7.0591 - val_acc: 0.3127\n",
      "Epoch 266/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5464 - acc: 0.7188Epoch 00265: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.5461 - acc: 0.7189 - val_loss: 7.0503 - val_acc: 0.3093\n",
      "Epoch 267/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5391 - acc: 0.7206Epoch 00266: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 204s - loss: 1.5405 - acc: 0.7203 - val_loss: 7.0553 - val_acc: 0.3115\n",
      "Epoch 268/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5409 - acc: 0.7183Epoch 00267: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.5399 - acc: 0.7187 - val_loss: 7.0664 - val_acc: 0.3105\n",
      "Epoch 269/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5372 - acc: 0.7190Epoch 00268: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.5357 - acc: 0.7192 - val_loss: 7.0529 - val_acc: 0.3103\n",
      "Epoch 270/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5354 - acc: 0.7196Epoch 00269: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.5348 - acc: 0.7197 - val_loss: 7.0546 - val_acc: 0.3103\n",
      "Epoch 271/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5315 - acc: 0.7217Epoch 00270: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.5312 - acc: 0.7218 - val_loss: 7.0552 - val_acc: 0.3111\n",
      "Epoch 272/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5313 - acc: 0.7191Epoch 00271: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 221s - loss: 1.5300 - acc: 0.7192 - val_loss: 7.0610 - val_acc: 0.3097\n",
      "Epoch 273/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5295 - acc: 0.7238Epoch 00272: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.5285 - acc: 0.7240 - val_loss: 7.0579 - val_acc: 0.3099\n",
      "Epoch 274/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5309 - acc: 0.7231Epoch 00273: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.5297 - acc: 0.7235 - val_loss: 7.0693 - val_acc: 0.3123\n",
      "Epoch 275/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5260 - acc: 0.7224Epoch 00274: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.5264 - acc: 0.7222 - val_loss: 7.0664 - val_acc: 0.3103\n",
      "Epoch 276/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5193 - acc: 0.7239Epoch 00275: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.5194 - acc: 0.7239 - val_loss: 7.0664 - val_acc: 0.3101\n",
      "Epoch 277/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5155 - acc: 0.7209Epoch 00276: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.5164 - acc: 0.7207 - val_loss: 7.0585 - val_acc: 0.3087\n",
      "Epoch 278/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5196 - acc: 0.7227Epoch 00277: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.5204 - acc: 0.7227 - val_loss: 7.0718 - val_acc: 0.3103\n",
      "Epoch 279/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5138 - acc: 0.7242Epoch 00278: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.5141 - acc: 0.7242 - val_loss: 7.0714 - val_acc: 0.3095\n",
      "Epoch 280/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5121 - acc: 0.7270Epoch 00279: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 213s - loss: 1.5112 - acc: 0.7271 - val_loss: 7.0728 - val_acc: 0.3099\n",
      "Epoch 281/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5085 - acc: 0.7274Epoch 00280: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 213s - loss: 1.5086 - acc: 0.7274 - val_loss: 7.0621 - val_acc: 0.3091\n",
      "Epoch 282/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5065 - acc: 0.7275Epoch 00281: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.5058 - acc: 0.7277 - val_loss: 7.0711 - val_acc: 0.3093\n",
      "Epoch 283/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5035 - acc: 0.7263Epoch 00282: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 210s - loss: 1.5037 - acc: 0.7260 - val_loss: 7.0672 - val_acc: 0.3101\n",
      "Epoch 284/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.5032 - acc: 0.7263Epoch 00283: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.5027 - acc: 0.7264 - val_loss: 7.0763 - val_acc: 0.3099\n",
      "Epoch 285/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4977 - acc: 0.7284Epoch 00284: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.4988 - acc: 0.7281 - val_loss: 7.0734 - val_acc: 0.3093\n",
      "Epoch 286/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4929 - acc: 0.7285Epoch 00285: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.4945 - acc: 0.7280 - val_loss: 7.0807 - val_acc: 0.3103\n",
      "Epoch 287/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4931 - acc: 0.7269Epoch 00286: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 207s - loss: 1.4942 - acc: 0.7267 - val_loss: 7.0863 - val_acc: 0.3105\n",
      "Epoch 288/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4919 - acc: 0.7282Epoch 00287: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.4922 - acc: 0.7279 - val_loss: 7.0918 - val_acc: 0.3091\n",
      "Epoch 289/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4882 - acc: 0.7299Epoch 00288: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.4873 - acc: 0.7299 - val_loss: 7.0872 - val_acc: 0.3105\n",
      "Epoch 290/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4887 - acc: 0.7286Epoch 00289: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.4881 - acc: 0.7288 - val_loss: 7.0904 - val_acc: 0.3113\n",
      "Epoch 291/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4883 - acc: 0.7292Epoch 00290: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4873 - acc: 0.7294 - val_loss: 7.0920 - val_acc: 0.3091\n",
      "Epoch 292/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4842 - acc: 0.7320Epoch 00291: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.4863 - acc: 0.7315 - val_loss: 7.0994 - val_acc: 0.3101\n",
      "Epoch 293/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4833 - acc: 0.7292Epoch 00292: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.4828 - acc: 0.7293 - val_loss: 7.1017 - val_acc: 0.3103\n",
      "Epoch 294/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4757 - acc: 0.7303Epoch 00293: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4763 - acc: 0.7303 - val_loss: 7.0802 - val_acc: 0.3087\n",
      "Epoch 295/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4743 - acc: 0.7328Epoch 00294: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.4737 - acc: 0.7330 - val_loss: 7.1013 - val_acc: 0.3101\n",
      "Epoch 296/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4798 - acc: 0.7323Epoch 00295: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4806 - acc: 0.7322 - val_loss: 7.0984 - val_acc: 0.3087\n",
      "Epoch 297/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4721 - acc: 0.7341Epoch 00296: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.4715 - acc: 0.7342 - val_loss: 7.0912 - val_acc: 0.3087\n",
      "Epoch 298/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4688 - acc: 0.7337Epoch 00297: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4681 - acc: 0.7340 - val_loss: 7.0910 - val_acc: 0.3087\n",
      "Epoch 299/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4667 - acc: 0.7343Epoch 00298: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.4658 - acc: 0.7345 - val_loss: 7.0997 - val_acc: 0.3097\n",
      "Epoch 300/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4664 - acc: 0.7343Epoch 00299: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.4653 - acc: 0.7344 - val_loss: 7.0908 - val_acc: 0.3087\n",
      "Epoch 301/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4670 - acc: 0.7347Epoch 00300: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.4654 - acc: 0.7351 - val_loss: 7.0937 - val_acc: 0.3103\n",
      "Epoch 302/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4612 - acc: 0.7364Epoch 00301: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.4622 - acc: 0.7362 - val_loss: 7.1150 - val_acc: 0.3095\n",
      "Epoch 303/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4586 - acc: 0.7337Epoch 00302: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 208s - loss: 1.4593 - acc: 0.7337 - val_loss: 7.1066 - val_acc: 0.3097\n",
      "Epoch 304/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4564 - acc: 0.7350Epoch 00303: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4568 - acc: 0.7349 - val_loss: 7.1048 - val_acc: 0.3079\n",
      "Epoch 305/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4536 - acc: 0.7347Epoch 00304: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.4540 - acc: 0.7347 - val_loss: 7.1107 - val_acc: 0.3089\n",
      "Epoch 306/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4540 - acc: 0.7348Epoch 00305: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 208s - loss: 1.4554 - acc: 0.7343 - val_loss: 7.1200 - val_acc: 0.3093\n",
      "Epoch 307/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4509 - acc: 0.7364Epoch 00306: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.4511 - acc: 0.7364 - val_loss: 7.1079 - val_acc: 0.3085\n",
      "Epoch 308/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.7370Epoch 00307: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.4459 - acc: 0.7369 - val_loss: 7.1079 - val_acc: 0.3089\n",
      "Epoch 309/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4449 - acc: 0.7388Epoch 00308: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 209s - loss: 1.4448 - acc: 0.7389 - val_loss: 7.1240 - val_acc: 0.3093\n",
      "Epoch 310/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4448 - acc: 0.7386Epoch 00309: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.4437 - acc: 0.7387 - val_loss: 7.1120 - val_acc: 0.3073\n",
      "Epoch 311/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4390 - acc: 0.7406Epoch 00310: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4389 - acc: 0.7406 - val_loss: 7.1271 - val_acc: 0.3089\n",
      "Epoch 312/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4423 - acc: 0.7388Epoch 00311: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.4423 - acc: 0.7387 - val_loss: 7.1118 - val_acc: 0.3087\n",
      "Epoch 313/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4356 - acc: 0.7393Epoch 00312: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.4358 - acc: 0.7392 - val_loss: 7.1185 - val_acc: 0.3087\n",
      "Epoch 314/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4381 - acc: 0.7412Epoch 00313: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4383 - acc: 0.7411 - val_loss: 7.1233 - val_acc: 0.3083\n",
      "Epoch 315/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4344 - acc: 0.7396Epoch 00314: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4330 - acc: 0.7399 - val_loss: 7.1170 - val_acc: 0.3081\n",
      "Epoch 316/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4358 - acc: 0.7404Epoch 00315: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.4357 - acc: 0.7404 - val_loss: 7.1258 - val_acc: 0.3077\n",
      "Epoch 317/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4326 - acc: 0.7409Epoch 00316: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4308 - acc: 0.7412 - val_loss: 7.1229 - val_acc: 0.3085\n",
      "Epoch 318/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4266 - acc: 0.7419Epoch 00317: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.4264 - acc: 0.7421 - val_loss: 7.1292 - val_acc: 0.3091\n",
      "Epoch 319/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4285 - acc: 0.7407Epoch 00318: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.4277 - acc: 0.7410 - val_loss: 7.1256 - val_acc: 0.3087\n",
      "Epoch 320/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 1.4221 - acc: 0.7434Epoch 00319: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 212s - loss: 1.4220 - acc: 0.7436 - val_loss: 7.1289 - val_acc: 0.3093\n",
      "Epoch 321/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4231 - acc: 0.7423Epoch 00320: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 204s - loss: 1.4235 - acc: 0.7424 - val_loss: 7.1401 - val_acc: 0.3087\n",
      "Epoch 322/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4165 - acc: 0.7432Epoch 00321: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4171 - acc: 0.7430 - val_loss: 7.1459 - val_acc: 0.3095\n",
      "Epoch 323/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4185 - acc: 0.7435Epoch 00322: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.4184 - acc: 0.7435 - val_loss: 7.1346 - val_acc: 0.3079\n",
      "Epoch 324/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4167 - acc: 0.7431Epoch 00323: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.4181 - acc: 0.7428 - val_loss: 7.1330 - val_acc: 0.3089\n",
      "Epoch 325/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4147 - acc: 0.7431Epoch 00324: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.4152 - acc: 0.7430 - val_loss: 7.1471 - val_acc: 0.3093\n",
      "Epoch 326/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4073 - acc: 0.7434Epoch 00325: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.4077 - acc: 0.7433 - val_loss: 7.1442 - val_acc: 0.3075\n",
      "Epoch 327/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4137 - acc: 0.7457Epoch 00326: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.4137 - acc: 0.7455 - val_loss: 7.1354 - val_acc: 0.3087\n",
      "Epoch 328/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4062 - acc: 0.7477Epoch 00327: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.4065 - acc: 0.7477 - val_loss: 7.1447 - val_acc: 0.3089\n",
      "Epoch 329/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4057 - acc: 0.7458Epoch 00328: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.4060 - acc: 0.7457 - val_loss: 7.1430 - val_acc: 0.3069\n",
      "Epoch 330/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4078 - acc: 0.7446Epoch 00329: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.4083 - acc: 0.7445 - val_loss: 7.1421 - val_acc: 0.3083\n",
      "Epoch 331/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4021 - acc: 0.7471Epoch 00330: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 207s - loss: 1.4016 - acc: 0.7472 - val_loss: 7.1415 - val_acc: 0.3087\n",
      "Epoch 332/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4000 - acc: 0.7463Epoch 00331: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.4012 - acc: 0.7462 - val_loss: 7.1537 - val_acc: 0.3089\n",
      "Epoch 333/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.4019 - acc: 0.7463Epoch 00332: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.4016 - acc: 0.7463 - val_loss: 7.1413 - val_acc: 0.3081\n",
      "Epoch 334/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3969 - acc: 0.7482Epoch 00333: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.3977 - acc: 0.7479 - val_loss: 7.1452 - val_acc: 0.3091\n",
      "Epoch 335/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3953 - acc: 0.7477Epoch 00334: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3946 - acc: 0.7478 - val_loss: 7.1484 - val_acc: 0.3081\n",
      "Epoch 336/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3937 - acc: 0.7484Epoch 00335: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3940 - acc: 0.7483 - val_loss: 7.1563 - val_acc: 0.3081\n",
      "Epoch 337/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3903 - acc: 0.7484Epoch 00336: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3913 - acc: 0.7482 - val_loss: 7.1559 - val_acc: 0.3079\n",
      "Epoch 338/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3906 - acc: 0.7496Epoch 00337: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3908 - acc: 0.7496 - val_loss: 7.1568 - val_acc: 0.3091\n",
      "Epoch 339/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3851 - acc: 0.7517Epoch 00338: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 214s - loss: 1.3846 - acc: 0.7518 - val_loss: 7.1549 - val_acc: 0.3087\n",
      "Epoch 340/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3829 - acc: 0.7501Epoch 00339: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 207s - loss: 1.3840 - acc: 0.7500 - val_loss: 7.1579 - val_acc: 0.3095\n",
      "Epoch 341/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3841 - acc: 0.7507Epoch 00340: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.3858 - acc: 0.7503 - val_loss: 7.1531 - val_acc: 0.3071\n",
      "Epoch 342/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3863 - acc: 0.7489Epoch 00341: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3871 - acc: 0.7487 - val_loss: 7.1674 - val_acc: 0.3081\n",
      "Epoch 343/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3770 - acc: 0.7512Epoch 00342: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3762 - acc: 0.7513 - val_loss: 7.1566 - val_acc: 0.3083\n",
      "Epoch 344/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3812 - acc: 0.7504Epoch 00343: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.3800 - acc: 0.7505 - val_loss: 7.1705 - val_acc: 0.3083\n",
      "Epoch 345/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3778 - acc: 0.7530Epoch 00344: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3771 - acc: 0.7532 - val_loss: 7.1599 - val_acc: 0.3077\n",
      "Epoch 346/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3774 - acc: 0.7517Epoch 00345: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.3784 - acc: 0.7515 - val_loss: 7.1711 - val_acc: 0.3077\n",
      "Epoch 347/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3770 - acc: 0.7504Epoch 00346: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.3771 - acc: 0.7504 - val_loss: 7.1739 - val_acc: 0.3083\n",
      "Epoch 348/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3712 - acc: 0.7523Epoch 00347: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3717 - acc: 0.7522 - val_loss: 7.1752 - val_acc: 0.3087\n",
      "Epoch 349/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3718 - acc: 0.7504Epoch 00348: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.3715 - acc: 0.7507 - val_loss: 7.1650 - val_acc: 0.3069\n",
      "Epoch 350/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3643 - acc: 0.7531Epoch 00349: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3647 - acc: 0.7531 - val_loss: 7.1661 - val_acc: 0.3071\n",
      "Epoch 351/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3620 - acc: 0.7545Epoch 00350: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3609 - acc: 0.7547 - val_loss: 7.1651 - val_acc: 0.3075\n",
      "Epoch 352/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3651 - acc: 0.7525Epoch 00351: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.3652 - acc: 0.7526 - val_loss: 7.1689 - val_acc: 0.3081\n",
      "Epoch 353/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3614 - acc: 0.7545Epoch 00352: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3615 - acc: 0.7544 - val_loss: 7.1782 - val_acc: 0.3073\n",
      "Epoch 354/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3615 - acc: 0.7553Epoch 00353: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.3602 - acc: 0.7554 - val_loss: 7.1685 - val_acc: 0.3073\n",
      "Epoch 355/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3580 - acc: 0.7553Epoch 00354: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 195s - loss: 1.3590 - acc: 0.7552 - val_loss: 7.1772 - val_acc: 0.3085\n",
      "Epoch 356/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3602 - acc: 0.7549Epoch 00355: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3611 - acc: 0.7548 - val_loss: 7.1718 - val_acc: 0.3075\n",
      "Epoch 357/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3570 - acc: 0.7544Epoch 00356: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3565 - acc: 0.7545 - val_loss: 7.1771 - val_acc: 0.3069\n",
      "Epoch 358/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3532 - acc: 0.7557Epoch 00357: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3540 - acc: 0.7557 - val_loss: 7.1757 - val_acc: 0.3069\n",
      "Epoch 359/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3561 - acc: 0.7552Epoch 00358: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.3547 - acc: 0.7556 - val_loss: 7.1877 - val_acc: 0.3071\n",
      "Epoch 360/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3445 - acc: 0.7578Epoch 00359: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 204s - loss: 1.3462 - acc: 0.7573 - val_loss: 7.1815 - val_acc: 0.3069\n",
      "Epoch 361/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3536 - acc: 0.7557Epoch 00360: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.3522 - acc: 0.7559 - val_loss: 7.1721 - val_acc: 0.3065\n",
      "Epoch 362/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3478 - acc: 0.7565Epoch 00361: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3476 - acc: 0.7565 - val_loss: 7.1761 - val_acc: 0.3081\n",
      "Epoch 363/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3454 - acc: 0.7584Epoch 00362: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.3462 - acc: 0.7582 - val_loss: 7.1873 - val_acc: 0.3075\n",
      "Epoch 364/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3404 - acc: 0.7582Epoch 00363: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.3410 - acc: 0.7579 - val_loss: 7.1925 - val_acc: 0.3087\n",
      "Epoch 365/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3408 - acc: 0.7576Epoch 00364: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 209s - loss: 1.3402 - acc: 0.7578 - val_loss: 7.1930 - val_acc: 0.3065\n",
      "Epoch 366/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3394 - acc: 0.7571Epoch 00365: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 198s - loss: 1.3376 - acc: 0.7575 - val_loss: 7.1864 - val_acc: 0.3060\n",
      "Epoch 367/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3420 - acc: 0.7575Epoch 00366: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 209s - loss: 1.3414 - acc: 0.7575 - val_loss: 7.1868 - val_acc: 0.3071\n",
      "Epoch 368/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3349 - acc: 0.7594Epoch 00367: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3349 - acc: 0.7594 - val_loss: 7.1948 - val_acc: 0.3077\n",
      "Epoch 369/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3345 - acc: 0.7594Epoch 00368: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3346 - acc: 0.7593 - val_loss: 7.1855 - val_acc: 0.3081\n",
      "Epoch 370/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3367 - acc: 0.7588Epoch 00369: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 199s - loss: 1.3347 - acc: 0.7593 - val_loss: 7.1870 - val_acc: 0.3071\n",
      "Epoch 371/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3309 - acc: 0.7606Epoch 00370: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 201s - loss: 1.3308 - acc: 0.7606 - val_loss: 7.2002 - val_acc: 0.3060\n",
      "Epoch 372/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3306 - acc: 0.7579Epoch 00371: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 210s - loss: 1.3306 - acc: 0.7580 - val_loss: 7.1890 - val_acc: 0.3054\n",
      "Epoch 373/400\n",
      "670/672 [============================>.] - ETA: 35s - loss: 1.3265 - acc: 0.7607Epoch 00372: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 11973s - loss: 1.3264 - acc: 0.7608 - val_loss: 7.1935 - val_acc: 0.3065\n",
      "Epoch 374/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3250 - acc: 0.7629Epoch 00373: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 196s - loss: 1.3243 - acc: 0.7629 - val_loss: 7.2093 - val_acc: 0.3077\n",
      "Epoch 375/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3236 - acc: 0.7622Epoch 00374: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 202s - loss: 1.3240 - acc: 0.7621 - val_loss: 7.1956 - val_acc: 0.3058\n",
      "Epoch 376/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3247 - acc: 0.7631Epoch 00375: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 210s - loss: 1.3246 - acc: 0.7631 - val_loss: 7.1950 - val_acc: 0.3056\n",
      "Epoch 377/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3286 - acc: 0.7611Epoch 00376: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.3290 - acc: 0.7611 - val_loss: 7.2103 - val_acc: 0.3065\n",
      "Epoch 378/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/672 [============================>.] - ETA: 0s - loss: 1.3246 - acc: 0.7619Epoch 00377: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.3243 - acc: 0.7620 - val_loss: 7.1981 - val_acc: 0.3063\n",
      "Epoch 379/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3231 - acc: 0.7621Epoch 00378: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.3220 - acc: 0.7624 - val_loss: 7.2079 - val_acc: 0.3071\n",
      "Epoch 380/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3202 - acc: 0.7618Epoch 00379: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 200s - loss: 1.3200 - acc: 0.7620 - val_loss: 7.2010 - val_acc: 0.3071\n",
      "Epoch 381/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3152 - acc: 0.7627Epoch 00380: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 197s - loss: 1.3160 - acc: 0.7623 - val_loss: 7.2021 - val_acc: 0.3065\n",
      "Epoch 382/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3147 - acc: 0.7643Epoch 00381: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.3141 - acc: 0.7644 - val_loss: 7.1983 - val_acc: 0.3062\n",
      "Epoch 383/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3174 - acc: 0.7622Epoch 00382: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 206s - loss: 1.3171 - acc: 0.7624 - val_loss: 7.1997 - val_acc: 0.3065\n",
      "Epoch 384/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3076 - acc: 0.7666Epoch 00383: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.3086 - acc: 0.7666 - val_loss: 7.2106 - val_acc: 0.3058\n",
      "Epoch 385/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3061 - acc: 0.7654Epoch 00384: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 205s - loss: 1.3062 - acc: 0.7654 - val_loss: 7.2050 - val_acc: 0.3058\n",
      "Epoch 386/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3102 - acc: 0.7632Epoch 00385: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.3097 - acc: 0.7633 - val_loss: 7.2281 - val_acc: 0.3069\n",
      "Epoch 387/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3078 - acc: 0.7648Epoch 00386: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 208s - loss: 1.3084 - acc: 0.7647 - val_loss: 7.2127 - val_acc: 0.3073\n",
      "Epoch 388/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3079 - acc: 0.7643Epoch 00387: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 203s - loss: 1.3082 - acc: 0.7642 - val_loss: 7.2147 - val_acc: 0.3065\n",
      "Epoch 389/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3051 - acc: 0.7660Epoch 00388: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 210s - loss: 1.3048 - acc: 0.7660 - val_loss: 7.2237 - val_acc: 0.3069\n",
      "Epoch 390/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3038 - acc: 0.7662Epoch 00389: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 214s - loss: 1.3044 - acc: 0.7660 - val_loss: 7.2120 - val_acc: 0.3069\n",
      "Epoch 391/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.3105 - acc: 0.7643Epoch 00390: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 213s - loss: 1.3091 - acc: 0.7647 - val_loss: 7.2148 - val_acc: 0.3060\n",
      "Epoch 392/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2997 - acc: 0.7658Epoch 00391: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 228s - loss: 1.3002 - acc: 0.7657 - val_loss: 7.2033 - val_acc: 0.3060\n",
      "Epoch 393/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2955 - acc: 0.7680Epoch 00392: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 213s - loss: 1.2977 - acc: 0.7674 - val_loss: 7.2196 - val_acc: 0.3063\n",
      "Epoch 394/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2958 - acc: 0.7688Epoch 00393: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 194s - loss: 1.2960 - acc: 0.7689 - val_loss: 7.2194 - val_acc: 0.3060\n",
      "Epoch 395/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2984 - acc: 0.7643Epoch 00394: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 193s - loss: 1.2980 - acc: 0.7644 - val_loss: 7.2141 - val_acc: 0.3069\n",
      "Epoch 396/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2924 - acc: 0.7698Epoch 00395: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.2919 - acc: 0.7697 - val_loss: 7.2161 - val_acc: 0.3058\n",
      "Epoch 397/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2937 - acc: 0.7681Epoch 00396: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.2938 - acc: 0.7679 - val_loss: 7.2242 - val_acc: 0.3069\n",
      "Epoch 398/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2883 - acc: 0.7693Epoch 00397: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.2882 - acc: 0.7694 - val_loss: 7.2061 - val_acc: 0.3063\n",
      "Epoch 399/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2904 - acc: 0.7695Epoch 00398: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.2902 - acc: 0.7697 - val_loss: 7.2255 - val_acc: 0.3065\n",
      "Epoch 400/400\n",
      "670/672 [============================>.] - ETA: 0s - loss: 1.2828 - acc: 0.7687Epoch 00399: saving model to SummarizationWithoutAttentionV3.1Weights.hdf5\n",
      "672/672 [==============================] - 192s - loss: 1.2846 - acc: 0.7683 - val_loss: 7.2204 - val_acc: 0.3056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69d5f3450>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[dataIX,decoderInputSummary], \n",
    "          y=decoderTargetSummary,\n",
    "          batch_size=2,\n",
    "          epochs=400,\n",
    "          validation_split=0.2,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Inference\n",
    "encoder_model_inf=Model(inputs=encoder_input,outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Inference\n",
    "decoder_state_input_h=Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input,initial_state=decoder_input_states)\n",
    "decoder_states=[decoder_h,decoder_c]\n",
    "decoder_inf_final_out = decoder_dense(decoder_out)\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_inf_final_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4867"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modiefiedSummaryWord_index['SOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab_summaries = {}\n",
    "for word, value in modiefiedSummaryWord_index.items():\n",
    "    int_to_vocab_summaries[value] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    # Initial states value is coming from the encoder \n",
    "    #We get the encoder states into states_val variable\n",
    "    states_val = encoder_model_inf.predict(input_seq)#return encoder states\n",
    "    target_seq = np.zeros((1,1,ModifiedVocabSize))\n",
    "    print('target_seq shape:->',target_seq.shape)\n",
    "    target_seq[0, 0, modiefiedSummaryWord_index['SOS']] = 1\n",
    "    print(target_seq.shape)\n",
    "    #target_seq=embeddingModifiedSummaries[modiefiedSummaryWord_index['SOS']]\n",
    "    summarized_sent = ''\n",
    "    stop_condition = False\n",
    "    i=1\n",
    "    while not stop_condition:\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        #print(decoder_out)\n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_summary_word = int_to_vocab_summaries[max_val_index]\n",
    "        #print('sampled_summary_word is:->',sampled_summary_word)\n",
    "        #print()\n",
    "        summarized_sent += sampled_summary_word+\" \"\n",
    "        #print('summarized_sent is:->',summarized_sent)\n",
    "        #print()\n",
    "        if ((sampled_summary_word == 'EOS') or (len(summarized_sent) >= maxSummaryLength)) :\n",
    "            print('terminated')\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1,1,ModifiedVocabSize))\n",
    "        target_seq[0, 0, max_val_index]=1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        i=i+1\n",
    "        \n",
    "    return summarized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a powerful documentarylike of \n",
      "Human Summary  a powerful documentarylike examination of the response to an occupying force the battle of algiers has not aged a bit since its release in 1966 PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though and repetitive and his \n",
      "Human Summary  poor plot development and slow pacing keep 54 from capturing the energy of it is legendary namesake PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is nasty closely to the 1984 \n",
      "Human Summary  while it hews closely to the 1984 original craig brewer infuses his footloose remake with toetapping energy and manages to keep the story fresh for a new generation PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though acted and an searing in \n",
      "Human Summary  tender funny and touching the sessions provides an acting showcase for its talented stars and proves it is possible for hollywood to produce a grownup movie about sex PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: this chreaus of rendering of joseph \n",
      "Human Summary  patrice chreaus exquisite rendering of joseph conrads the return brings underlying passions to surface in a longsuffering marriage PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the tunnel is a a tensionfilled \n",
      "Human Summary  too over the top for its own good but ultimately rescued by the casts charm director john landis grace and several soulstirring musical numbers PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though infectiously intelligently \n",
      "Human Summary  an infectiously fun blend of special effects and comedy with bill murrays hilarious deadpan performance leading a cast of great comic turns PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though of fires performances and \n",
      "Human Summary  small in scale but large in impact boy as career making performances particularly that by star andrew garfield and carefully crafted characters defy judgment and aggressively provoke debate PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is an very than this little \n",
      "Human Summary  only the very young will get the most out of this silly trifle PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the regurgitates of points of \n",
      "Human Summary  it regurgitates plot points from earlier animated efforts and is not quite as funny as it should be but a topshelf voice cast and strong visuals help make megamind a \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a strong \n",
      "Human Summary  hal ashbys comedy is too dark and twisted for some and occasionally oversteps its bounds but there is no denying the films warm humor and big heart PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is a dark and a a injected \n",
      "Human Summary  it is a film about a guy injected with speed wait there is no bus it is a film about a guy who has to kick a bunch of squirmy \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the resemblance of the \n",
      "Human Summary  bearing little resemblance to the 1953 original house of wax is a formulaic but betterthanaverage teen slasher flick PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the landmark of hook and a huge \n",
      "Human Summary  gags are not that funny PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the tunnel is a a tensionfilled \n",
      "Human Summary  despite the best efforts of its competent cast underworld rise of the lycans is an indistinguishable and unnecessary prequel PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the visually sincerity \n",
      "Human Summary  rich in sweet sincerity intelligence and good oldfashioned inspirational drama october sky is a comingofage story with a heart to match its hollywood craftsmanship PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: despite je taime is neither but \n",
      "Human Summary  paris je taime is uneven but there are more than enough delightful moments in this omnibus tribute to the city of lights to tip the scale in its favor PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though it a captivating performance \n",
      "Human Summary  thanks to a captivating performance from jeff bridges crazy heart transcends its overly familiar origins and finds new meaning in an old story PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: woody patric and frenetic liotta \n",
      "Human Summary  jason patric and ray liotta are electrifying in this gritty if a little too familiar cop drama PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though emmerich and 1969 as recently \n",
      "Human Summary  roland emmerich delivers his trademark visual and emotional bombast but the more anonymous stops and tries to convince the audience of its halfbaked theory the less convincing it becomes PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the the movie is a first cast \n",
      "Human Summary  audiences will need to tolerate a certain amount of narrative drift but thanks to sensitive direction from noah baumbach and an endearing performance from greta gerwig frances ha makes it \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a powerful and drama about starcrossed \n",
      "Human Summary  an upfront study of a drug addict confronting his demons oslo august 31st makes this dark journey worthwhile with fantastic directing and equally fantastic acting PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it it is not quite but it as ever \n",
      "Human Summary  if audiences walk away from this subversive surreal shocker not fully understanding the story they might also walk away with a deeper perception of the potential of film storytelling PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though by the horse performance \n",
      "Human Summary  pivoting on the unusual relationship between seasoned hitman and his 12yearold apprentice a breakout turn by young natalie portman luc bessons lon is a stylish and oddly affecting thriller PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the queen of versailles is a timely \n",
      "Human Summary  the queen of versailles is a timely engaging and richly drawn portrait of the american dream improbably composed of equal parts compassion and schadenfreude PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though 51 squanders the victor \n",
      "Human Summary  planet 51 squanders an interesting premise with an overly familiar storyline stock characters and humor that alternates between curious and potentially offensive PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though in the heartfelt in sessions \n",
      "Human Summary  in addition to its breathtaking underwater photography sharkwater has a convincing impassioned argument of how the plight of sharks affects everyone PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a anarchic \n",
      "Human Summary  better off dead is an anarchic mix of black humor and surreal comedy anchored by john cusacks winsome charming performance PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated\n",
      "System Generated Summary: the landmark of hook of a huge \n",
      "Human Summary  while fast times at ridgemont high features sean penns legendary performance the film endures because it accurately captured the small details of school work and teenage life PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: an longoverdue of japanese perros \n",
      "Human Summary  this longoverdue return from alejandro jodorowsky finds him just as overflowing with imagination and heart as fans have come to expect PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a few rousing thriller that that \n",
      "Human Summary  a few rousing action sequences cannot make up for pathfinders nonexistent plot and silly dialogue PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though it is a pleasures to reflect \n",
      "Human Summary  though not without its pleasures never back down faithfully adheres to every imaginable fight movie clich PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  ambitious adventurous and sometimes frightening pinocchio arguably represents the pinnacle of disneys collected works it is beautifully crafted and emotionally resonant PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: essentially a sports performance \n",
      "Human Summary  essentially a sports movie with drums the energetic drumline somehow manages to make the familiar seem fresh PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the mainstream and deeply \n",
      "Human Summary  a feelgood success crafted with care kim mordaunts story of two young kids in laos is a heartfelt audience pleaser while remaining sensitive toward its subjects PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: disturbing neeson and entirely \n",
      "Human Summary  disturbing controversial but entirely engrossing hard candy is well written with strong lead performances especially that of newcomer ellen page a movie that stays with the viewer long after leaving \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: mission and rogue emotionally \n",
      "Human Summary  sadistic violence and rote humor saddle a predictable action premise PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: strikingly stark brutally instincts \n",
      "Human Summary  strikingly stark brutally honest and rivetingly assembled the gatekeepers offers essential perspective on a seemingly intractable war from some of the men who fought it PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the freshness of the effects \n",
      "Human Summary  led by powerful performances from kristen wiig and bill hader the skeleton twins effectively mines laughs and tears from family drama PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a ridiculous of hook that potboiler \n",
      "Human Summary  oliver company is a decidedly lesser effort in the disney canon with lackluster songs stiff animation and a thoroughly predictable plot PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  don chaffeys jason and the argonauts is an outlandish transportive piece of nostalgia whose real star is the masterful stopmotion animation work of ray harryhausen PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a a twee intellectual exercise \n",
      "Human Summary  both an intriguing intellectual exercise and an amusing look at the contrasts between the two filmmakers PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: roland emmerichs 2012 and tasteless \n",
      "Human Summary  roland emmerichs 2012 provides plenty of visual thrills but lacks a strong enough script to support its massive scope and inflated length PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is not quite as groundbreaking \n",
      "Human Summary  it is not deep or groundbreaking but what it lacks in profundity puss in boots more than makes up for with an abundance of wit visual sparkle and effervescent charm \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: an loving and meticulous sendup \n",
      "Human Summary  a loving and meticulous sendup of 1970s blaxsploitation movies black dynamite is funny enough for the frat house and clever enough for film buffs PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a strong \n",
      "Human Summary  resident evil extinction is more of the same its few impressive action sequences unable to compensate for the pedestrian plot PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the rundown is not as as as as \n",
      "Human Summary  the rundown does not break any new ground but it is a smart funny buddy action picture with terrific comic chemistry between dwayne the rock johnson and seann william scott \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the signs of probation is an movie \n",
      "Human Summary  the scifi thriller push is visually flashy but hyperkinetic and convoluted PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  a moving film with moments of humor liberty heights succeeds in capturing the feel of the 50s with great performances and sensitive direction PAD PAD PAD PAD PAD PAD PAD \n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a gutwrenching and kate the an \n",
      "Human Summary  matthew mcconaughey and kate hudson are charming together but they cannot overcome how to lose a guy in 10 days silly premise and predictable script PAD PAD PAD PAD PAD \n"
     ]
    }
   ],
   "source": [
    "human_summary=[]\n",
    "for i in range(50):    \n",
    "    #print('System Generated Summary:',summary)\n",
    "    temp=[]\n",
    "    for j in range(len(paddedSummary[i])):\n",
    "        temp.append(int_to_vocab_summaries[paddedSummary[i][j]])\n",
    "    human_summary.append(temp)    \n",
    "humanSummary=\" \"        \n",
    "for i in range(50):\n",
    "    data=dataIX[i].reshape(1,200)\n",
    "    summary=decode_seq(data)\n",
    "    print('System Generated Summary:',summary)\n",
    "    for j in range(len(human_summary[i])):\n",
    "        humanSummary+=human_summary[i][j]+\" \"\n",
    "    print('Human Summary',humanSummary)\n",
    "    humanSummary=\" \"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a powerful documentarylike of \n",
      "Human Summary  a powerful documentarylike examination of the response to an occupying force the battle of algiers has not aged a bit since its release in 1966 PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.330289129538\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though and repetitive and his \n",
      "Human Summary  poor plot development and slow pacing keep 54 from capturing the energy of it is legendary namesake PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.313112145543\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is nasty closely to the 1984 \n",
      "Human Summary  while it hews closely to the 1984 original craig brewer infuses his footloose remake with toetapping energy and manages to keep the story fresh for a new generation PAD PAD \n",
      "BlEU SCORE IS:-> 0.303239217432\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though acted and an searing in \n",
      "Human Summary  tender funny and touching the sessions provides an acting showcase for its talented stars and proves it is possible for hollywood to produce a grownup movie about sex PAD PAD \n",
      "BlEU SCORE IS:-> 0.282038037409\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: this chreaus of rendering of joseph \n",
      "Human Summary  patrice chreaus exquisite rendering of joseph conrads the return brings underlying passions to surface in a longsuffering marriage PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.307318148576\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the tunnel is a a tensionfilled \n",
      "Human Summary  too over the top for its own good but ultimately rescued by the casts charm director john landis grace and several soulstirring musical numbers PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.277350098113\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though infectiously intelligently \n",
      "Human Summary  an infectiously fun blend of special effects and comedy with bill murrays hilarious deadpan performance leading a cast of great comic turns PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.284472943338\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though of fires performances and \n",
      "Human Summary  small in scale but large in impact boy as career making performances particularly that by star andrew garfield and carefully crafted characters defy judgment and aggressively provoke debate PAD PAD \n",
      "BlEU SCORE IS:-> 0.292279208426\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is an very than this little \n",
      "Human Summary  only the very young will get the most out of this silly trifle PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.288675134595\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the regurgitates of points of \n",
      "Human Summary  it regurgitates plot points from earlier animated efforts and is not quite as funny as it should be but a topshelf voice cast and strong visuals help make megamind a \n",
      "BlEU SCORE IS:-> 0.289538141407\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a strong \n",
      "Human Summary  hal ashbys comedy is too dark and twisted for some and occasionally oversteps its bounds but there is no denying the films warm humor and big heart PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.305233847834\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is a dark and a a injected \n",
      "Human Summary  it is a film about a guy injected with speed wait there is no bus it is a film about a guy who has to kick a bunch of squirmy \n",
      "BlEU SCORE IS:-> 0.307389311747\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the resemblance of the \n",
      "Human Summary  bearing little resemblance to the 1953 original house of wax is a formulaic but betterthanaverage teen slasher flick PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.310460210283\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the landmark of hook and a huge \n",
      "Human Summary  gags are not that funny PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.296647939484\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the tunnel is a a tensionfilled \n",
      "Human Summary  despite the best efforts of its competent cast underworld rise of the lycans is an indistinguishable and unnecessary prequel PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.276533159377\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the visually sincerity \n",
      "Human Summary  rich in sweet sincerity intelligence and good oldfashioned inspirational drama october sky is a comingofage story with a heart to match its hollywood craftsmanship PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.272165526976\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: despite je taime is neither but \n",
      "Human Summary  paris je taime is uneven but there are more than enough delightful moments in this omnibus tribute to the city of lights to tip the scale in its favor PAD \n",
      "BlEU SCORE IS:-> 0.310086836473\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though it a captivating performance \n",
      "Human Summary  thanks to a captivating performance from jeff bridges crazy heart transcends its overly familiar origins and finds new meaning in an old story PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.295312211609\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: woody patric and frenetic liotta \n",
      "Human Summary  jason patric and ray liotta are electrifying in this gritty if a little too familiar cop drama PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.307562342615\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though emmerich and 1969 as recently \n",
      "Human Summary  roland emmerich delivers his trademark visual and emotional bombast but the more anonymous stops and tries to convince the audience of its halfbaked theory the less convincing it becomes PAD \n",
      "BlEU SCORE IS:-> 0.297559517856\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the the movie is a first cast \n",
      "Human Summary  audiences will need to tolerate a certain amount of narrative drift but thanks to sensitive direction from noah baumbach and an endearing performance from greta gerwig frances ha makes it \n",
      "BlEU SCORE IS:-> 0.262265264156\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a powerful and drama about starcrossed \n",
      "Human Summary  an upfront study of a drug addict confronting his demons oslo august 31st makes this dark journey worthwhile with fantastic directing and equally fantastic acting PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.29488391231\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it it is not quite but it as ever \n",
      "Human Summary  if audiences walk away from this subversive surreal shocker not fully understanding the story they might also walk away with a deeper perception of the potential of film storytelling PAD \n",
      "BlEU SCORE IS:-> 0.25264557632\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though by the horse performance \n",
      "Human Summary  pivoting on the unusual relationship between seasoned hitman and his 12yearold apprentice a breakout turn by young natalie portman luc bessons lon is a stylish and oddly affecting thriller PAD \n",
      "BlEU SCORE IS:-> 0.296021732275\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the queen of versailles is a timely \n",
      "Human Summary  the queen of versailles is a timely engaging and richly drawn portrait of the american dream improbably composed of equal parts compassion and schadenfreude PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.305624922751\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though 51 squanders the victor \n",
      "Human Summary  planet 51 squanders an interesting premise with an overly familiar storyline stock characters and humor that alternates between curious and potentially offensive PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.303821810125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though in the heartfelt in sessions \n",
      "Human Summary  in addition to its breathtaking underwater photography sharkwater has a convincing impassioned argument of how the plight of sharks affects everyone PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.274351630584\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a anarchic \n",
      "Human Summary  better off dead is an anarchic mix of black humor and surreal comedy anchored by john cusacks winsome charming performance PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.281546253186\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the landmark of hook of a huge \n",
      "Human Summary  while fast times at ridgemont high features sean penns legendary performance the film endures because it accurately captured the small details of school work and teenage life PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.282466341433\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: an longoverdue of japanese perros \n",
      "Human Summary  this longoverdue return from alejandro jodorowsky finds him just as overflowing with imagination and heart as fans have come to expect PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.295312211609\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a few rousing thriller that that \n",
      "Human Summary  a few rousing action sequences cannot make up for pathfinders nonexistent plot and silly dialogue PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.307147558417\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though it is a pleasures to reflect \n",
      "Human Summary  though not without its pleasures never back down faithfully adheres to every imaginable fight movie clich PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.303355467855\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  ambitious adventurous and sometimes frightening pinocchio arguably represents the pinnacle of disneys collected works it is beautifully crafted and emotionally resonant PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.26984353611\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: essentially a sports performance \n",
      "Human Summary  essentially a sports movie with drums the energetic drumline somehow manages to make the familiar seem fresh PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.304290309725\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the mainstream and deeply \n",
      "Human Summary  a feelgood success crafted with care kim mordaunts story of two young kids in laos is a heartfelt audience pleaser while remaining sensitive toward its subjects PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.309039832348\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: disturbing neeson and entirely \n",
      "Human Summary  disturbing controversial but entirely engrossing hard candy is well written with strong lead performances especially that of newcomer ellen page a movie that stays with the viewer long after leaving \n",
      "BlEU SCORE IS:-> 0.273861278753\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: mission and rogue emotionally \n",
      "Human Summary  sadistic violence and rote humor saddle a predictable action premise PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.294392028878\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: strikingly stark brutally instincts \n",
      "Human Summary  strikingly stark brutally honest and rivetingly assembled the gatekeepers offers essential perspective on a seemingly intractable war from some of the men who fought it PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.271448357015\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: though the freshness of the effects \n",
      "Human Summary  led by powerful performances from kristen wiig and bill hader the skeleton twins effectively mines laughs and tears from family drama PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.264906471413\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a ridiculous of hook that potboiler \n",
      "Human Summary  oliver company is a decidedly lesser effort in the disney canon with lackluster songs stiff animation and a thoroughly predictable plot PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.313473915907\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  don chaffeys jason and the argonauts is an outlandish transportive piece of nostalgia whose real star is the masterful stopmotion animation work of ray harryhausen PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.284747398726\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a a twee intellectual exercise \n",
      "Human Summary  both an intriguing intellectual exercise and an amusing look at the contrasts between the two filmmakers PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.283278861866\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: roland emmerichs 2012 and tasteless \n",
      "Human Summary  roland emmerichs 2012 provides plenty of visual thrills but lacks a strong enough script to support its massive scope and inflated length PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.315301767642\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: it is not quite as groundbreaking \n",
      "Human Summary  it is not deep or groundbreaking but what it lacks in profundity puss in boots more than makes up for with an abundance of wit visual sparkle and effervescent charm \n",
      "BlEU SCORE IS:-> 0.290408934776\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: an loving and meticulous sendup \n",
      "Human Summary  a loving and meticulous sendup of 1970s blaxsploitation movies black dynamite is funny enough for the frat house and clever enough for film buffs PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.305887645161\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the undiscovered is is a strong \n",
      "Human Summary  resident evil extinction is more of the same its few impressive action sequences unable to compensate for the pedestrian plot PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.289538141407\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the rundown is not as as as as \n",
      "Human Summary  the rundown does not break any new ground but it is a smart funny buddy action picture with terrific comic chemistry between dwayne the rock johnson and seann william scott \n",
      "BlEU SCORE IS:-> 0.273336136795\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: the signs of probation is an movie \n",
      "Human Summary  the scifi thriller push is visually flashy but hyperkinetic and convoluted PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.303488489333\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a whimsically of antiwar that \n",
      "Human Summary  a moving film with moments of humor liberty heights succeeds in capturing the feel of the 50s with great performances and sensitive direction PAD PAD PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.29617443888\n",
      "target_seq shape:-> (1, 1, 4870)\n",
      "(1, 1, 4870)\n",
      "terminated\n",
      "System Generated Summary: a gutwrenching and kate the an \n",
      "Human Summary  matthew mcconaughey and kate hudson are charming together but they cannot overcome how to lose a guy in 10 days silly premise and predictable script PAD PAD PAD PAD PAD \n",
      "BlEU SCORE IS:-> 0.286972021592\n",
      "AVERAGE BLEU SCORE:-> 0.29274190712\n"
     ]
    }
   ],
   "source": [
    "humanSummary=\" \"  \n",
    "scores=[]\n",
    "for i in range(50):\n",
    "    testData=testIX[i].reshape(1,200)\n",
    "    summary=decode_seq(testData)\n",
    "    print('System Generated Summary:',summary)\n",
    "    for j in range(len(human_summary[i])):\n",
    "        humanSummary+=human_summary[i][j]+\" \"\n",
    "    print('Human Summary',humanSummary)\n",
    "    #calculation of bleu score\n",
    "    score=sentence_bleu(summary,humanSummary,weights=(0.5, 0.5, 0, 0))\n",
    "    print('BlEU SCORE IS:->',score)\n",
    "    scores.append(score)      \n",
    "    humanSummary=\" \"\n",
    "\n",
    "total=0\n",
    "for i in scores:\n",
    "    total+=i\n",
    "print('AVERAGE BLEU SCORE:->',total/len(scores))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
